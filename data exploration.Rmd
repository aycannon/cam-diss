---
title: "Dissertation Code v1"
author: "Archie C"
date: "2025-05-22"
output: html_document
---

```{r preamble, include=FALSE, message=FALSE, warning=FALSE}
source("preamble.R")

expectregurl <- "https://cran.r-project.org/src/contrib/Archive/expectreg/expectreg_0.53.tar.gz"
# install.packages(expectregurl, repos=NULL, type="source")
```


```{r Data Import, attr.message=FALSE, warning=FALSE, message=FALSE}
df <- read_xlsx("data/data1.xlsx")
dt <- as.data.table(df)
```

```{r wide format}
data <- df %>%
    mutate(Base = gsub("=", "", Instrument))

wide <- data %>%
    distinct() %>% # there are duplicates, but they are perfect duplicates so we take the distinct values
    dplyr::select(Base, Date, "Mid Price") %>%
    pivot_wider(names_from = Base, values_from = "Mid Price") %>%
    arrange(Date) %>%
    mutate(across(-Date, log))
# everything has been logged as we work in log(spot) and log(forward)
# now we need s_{t+1}

spot_lagged <- wide %>%
  transmute(Date = Date %m+% months(1),  # shift date back by 1 month to align with t
            GBP_t1 = GBP,
            EUR_t1 = EUR,
            JPY_t1 = JPY,
            CAD_t1 = CAD,
            GBP_FWD = GBP1MV,
            EUR_FWD = EUR1MV,
            JPY_FWD = JPY1MV,
            CAD_FWD = CAD1MV)

wide_data <- wide %>%
    inner_join(spot_lagged, by = "Date") %>%
    mutate(GBP_excess = GBP - GBP_t1,
           EUR_excess = EUR - EUR_t1,
           JPY_excess = JPY - JPY_t1,
           CAD_excess = CAD - CAD_t1,
           GBP_prem = GBP_FWD - GBP_t1,
           EUR_prem = EUR_FWD - EUR_t1,
           JPY_prem = JPY_FWD - JPY_t1,
           CAD_prem = CAD_FWD - CAD_t1)

```

```{r Monthly Data}
monthly_data <- data %>%
  mutate(
    YearMonth = floor_date(Date, "month"),
    Type      = ifelse(grepl("1MV$", Base), "forward", "spot"),
    Currency  = gsub("1MV$", "", Base)
  ) %>%
  filter(Type %in% c("spot","forward")) %>%
  group_by(YearMonth, Currency, Type) %>%
  summarise(
    Mid = mean(`Mid Price`, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from  = Type,
    values_from = Mid
  ) %>%
  # Now we already have exactly one (spot, forward) per month×currency.
  mutate(
    spot    = if_else(Currency %in% c("GBP","EUR"), 1/spot,    spot),
    forward = if_else(Currency %in% c("GBP","EUR"), 1/forward, forward)
  ) %>%
  mutate(
    across(c(spot, forward), log)
  )

monthly_nolog <- data %>%
  mutate(
    YearMonth = floor_date(Date, "month"),
    Type      = ifelse(grepl("1MV$", Base), "forward", "spot"),
    Currency  = gsub("1MV$", "", Base)
  ) %>%
  filter(Type %in% c("spot","forward")) %>%
  group_by(YearMonth, Currency, Type) %>%
  summarise(
    Mid = mean(`Mid Price`, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from  = Type,
    values_from = Mid
  ) %>%
  # Now we already have exactly one (spot, forward) per month×currency.
  mutate(
    spot    = if_else(Currency %in% c("GBP","EUR"), 1/spot,    spot),
    forward = if_else(Currency %in% c("GBP","EUR"), 1/forward, forward)
  ) 


wide_month <- monthly_data %>%
  pivot_wider(names_from = Currency, values_from = c(spot, forward), names_sep = "_") %>%
  arrange(YearMonth)

# Create lagged spot values to compute excess returns
spot_lagged_month <- wide_month %>%
  transmute(
    YearMonth = YearMonth %m+% months(1),
    spot_GBP_t1 = spot_GBP,
    spot_EUR_t1 = spot_EUR,
    spot_JPY_t1 = spot_JPY,
    spot_CAD_t1 = spot_CAD
  )

# Merge and compute returns
wide_month <- wide_month %>%
  inner_join(spot_lagged_month, by = "YearMonth") %>%
  mutate(
    GBP_excess = spot_GBP_t1 - spot_GBP,
    EUR_excess = spot_EUR_t1 - spot_EUR,
    JPY_excess = spot_JPY_t1 - spot_JPY,
    CAD_excess = spot_CAD_t1 - spot_CAD,
    GBP_prem   = forward_GBP - spot_GBP,
    EUR_prem   = forward_EUR - spot_EUR,
    JPY_prem   = forward_JPY - spot_JPY,
    CAD_prem   = forward_CAD - spot_CAD
  )



```

```{r Plotting Spot Rates}
ggplot(data = wide_month %>%
  dplyr::select(c("YearMonth","spot_CAD", "spot_EUR", "spot_GBP")) %>%
    pivot_longer(cols = -YearMonth, names_to = "Currency", values_to = "Spot"),
       aes(x = YearMonth, y = Spot, color = Currency)) +
  geom_line() +
    theme_minimal()

ggplot(data = wide_month %>%
  dplyr::select(c("YearMonth","spot_JPY")) %>%
    pivot_longer(cols = -YearMonth, names_to = "Currency", values_to = "Spot"),
       aes(x = YearMonth, y = Spot, color = Currency)) +
  geom_line() +
    theme_minimal()

```

```{r LM regressions with monthly data}
lm_data_monthly <- list()

for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))

  # Fit linear model
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)

  # Classical SEs
  model_summary <- summary(model)
  SE_classical <- model_summary$coefficients[, "Std. Error"]

  # Newey-West SEs (lag = 14)
  nw_vcov <- NeweyWest(model, lag = 14, prewhite = FALSE)
  nw_se   <- coeftest(model, vcov. = nw_vcov)

  # Save stats
  summary_stats <- tibble(
    Currency     = cur,
    Intercept    = coef(model)[1],
    SE_Intercept = SE_classical[1],
    SE_NW_Intercept = nw_se[1, 2],
    Slope        = coef(model)[2],
    SE_Slope     = SE_classical[2],
    SE_NW_Slope  = nw_se[2, 2],
    R2           = model_summary$r.squared,
    AIC          = AIC(model),
    BIC          = BIC(model),
    N            = nobs(model)
  )

  # Residuals and fitted values
  residuals_df <- tibble(
    Currency = cur,
    Fitted   = fitted(model),
    Residual = resid(model)
  )

  # Store everything
  lm_data_monthly[[cur]] <- list(
    model       = model,
    summary     = summary_stats,
    residuals   = residuals_df
  )
}

# Combine all regression summaries into one table
lmresults_df_monthly <- bind_rows(lapply(lm_data_monthly, `[[`, "summary"))
print(lmresults_df_monthly)
```

```{r data vis}

ggplot(monthly_nolog %>% filter(Currency != "JPY"), aes(x = YearMonth, y = spot, color = Currency)) +
    geom_line() +
    geom_hline(yintercept = 1, linetype = "dashed", color = "grey") +
    labs(title = "Spot Rates",
         x = "Date",
         y = "Spot Rate (x/USD)") +
    theme_minimal() +
    theme(legend.position = "bottom")
```

```{r Linear Model}
lm_data <- list()
for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)

  summary_stats <- tibble(
    Currency   = cur,
    Intercept  = coef(model)[1],
    SE.Intercept = summary(model)$coefficients[1, 2],
    Slope      = coef(model)[2],
    SE.Slope   = summary(model)$coefficients[2, 2],
    R2         = summary(model)$r.squared,
    N          = nobs(model)
  )
  
  fitted_df <- tibble(
    Currency = cur,
    Fitted   = fitted(model),
    Residual = resid(model)
  )
  
  lm_data[[cur]] <- list(summary = summary_stats, residuals = fitted_df)
}


lmresults_df <- bind_rows(list(lm_data$GBP$summary, lm_data$EUR$summary, lm_data$CAD$summary, lm_data$JPY$summary))
print(lmresults_df)

```

```{r Checking for autocorrelation in the errors}
for (cur in currencies) {
    cat("------", cur, "------\n")
  model <- lm(as.formula(paste0(cur, "_excess ~ ", cur, "_prem")), data = wide_data)
  print(cur)
  print(Box.test(resid(model), lag = 22, type = "Ljung-Box"))
}

par(mfrow = c(2, 2))  # 2x2 grid for 4 plots
for (cur in currencies) {
  model <- lm(as.formula(paste0(cur, "_excess ~ ", cur, "_prem")), data = wide_data)
  res <- resid(model)
  acf(res, lag.max = 40, main = paste("ACF of Residuals:", cur))
}

```

```{r estimate ARIMA for residuals}
library(forecast)

for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")

  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    dplyr::filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))

  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)
  res <- resid(model)

  cat("-----", cur, "-----\n")
  auto_model <- auto.arima(res, max.p = 0, max.q = 30, seasonal = FALSE, ic = "bic")
  print(auto_model)
}

```

```{r Newey-West Standard Errors - everything insig}

lmnw_data <- list()
for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)
  nw_vcov <- NeweyWest(model, lag = 14, prewhite = FALSE)
  nw_se <- coeftest(model, vcov. = nw_vcov)

  summary_stats <- tibble(
    Currency   = cur,
    Intercept  = coef(model)[1],
    SE.Intercept = nw_se[1, "Std. Error"],
    Slope      = coef(model)[2],
    SE.Slope   = nw_se[2, "Std. Error"],
    R2         = summary(model)$r.squared,
    N          = nobs(model)
  )
  
  fitted_df <- tibble(
    Currency = cur,
    Fitted   = fitted(model),
    Residual = resid(model)
  )
  
  lmnw_data[[cur]] <- list(summary = summary_stats, residuals = fitted_df)
}
lmnwresults_df <- bind_rows(list(lmnw_data$GBP$summary, lmnw_data$EUR$summary, lmnw_data$CAD$summary, lmnw_data$JPY$summary))
print(lmnwresults_df)

```


```{r Plotting lm Residuals}

# Combine all residuals
resid_plot_df <- bind_rows(list(lm_data$GBP$residuals,
                           lm_data$EUR$residuals,
                           lm_data$CAD$residuals,
                           lm_data$JPY$residuals))

means_sds <- resid_plot_df %>%
  group_by(Currency) %>%
  summarise(
    Mean = mean(Residual, na.rm = TRUE),
    SD   = sd(Residual,   na.rm = TRUE),
    .groups = "drop"
  )


normal_curves <- means_sds %>%
  mutate(x = map2(Mean, SD, ~ seq(.x - 4*.y, .x + 4*.y, length.out = 200))) %>%
  unnest(cols = c(x)) %>%
  mutate(y = dnorm(x, mean = Mean, sd = SD))

# Now plot
ggplot(resid_plot_df, aes(x = Residual)) +
  geom_density(color = "darkred", linewidth = 1, fill = "darkred", alpha = 0.5) +
  geom_line(data = normal_curves, aes(x = x, y = y), color = "blue", linetype = "dashed", linewidth = 0.5, alpha = 0.9) +
  facet_wrap(~ Currency, scales = "free") +
  theme_minimal() +
  labs(title = "Prediction Error Densities vs Fitted Normal Distribution",
       x = "Residual",
       y = "Density")

```

```{r simple regression to check on the forward premium bias}
currencies <- c("GBP", "EUR", "CAD", "JPY")

# Construct tidy summary for each currency
simple <- map_dfr(currencies, function(cur) {
  excess_var <- paste0(cur, "_excess")
  prem_var <- paste0(cur, "_prem")
  
  data_cur <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = data_cur)
  tidy(model) %>% mutate(Currency = cur)
})

for (base in currencies) {
  cat("--------------", base , "--------------\n")
  
  excess_var <- paste0(base, "_excess")
  prem_var <- paste0(base, "_prem")
  
  # Keep only relevant rows with no NA
  df <- wide_data %>%
    dplyr::select(Date, all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  # Dynamically construct the formula
  f <- as.formula(paste0(excess_var, " ~ ", prem_var))
  m <- lm(f, data = df)
  
  cat("  alpha:", round(coef(m)[1], 4), 
      " | beta:", round(coef(m)[2], 4), "\n")
  
  # Run hypothesis test: H0: beta = 1
  print(linearHypothesis(m, paste0(prem_var, " = 1")))
}
```


So we have found recreated some of the results. We see that in the case of Canada, the result is not statistically different from 1, but in the case of the other currencies, we see that the forward premium is statistically significantly different from 1. This is consistent with the findings in the literature.

```{r plot of coefficients from simple regression}
ggplot(lmresults_df, aes(x = Currency, y = Slope)) +
  geom_point() +
  geom_errorbar(aes(ymin = Slope - 1.96 * SE.Slope,
                    ymax = Slope + 1.96 * SE.Slope),
                width = 0.1) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey") +
  labs(title = "Simple Regression Estimates",
       x = "Currency", y = expression(hat(beta) ~ "(on forward premium)")) +
  theme_minimal()
```

# Estimating the asymmetric loss function

```{r estimating the loss functions}
estimate_a <- function(currencies, wide_data, p) {
  # Define moment function inside for clarity/scope
  gmm_moments <- function(theta, data) {
    alpha <- theta[1]
    e <- data$forecast_error
    z <- as.matrix(data[, c("const", "lag_error")])
    lambda <- alpha + (1 - 2 * alpha) * (e < 0)
    g <- lambda * e^(p - 1) * z
    return(g)
  }

  currency_results <- list()

  for (cur in currencies) {
    message("Running GMM for ", cur)

    e_col <- cur
    f_col <- paste0(cur, "_FWD")

    data_cur <- wide_data %>%
      transmute(
        forecast_error = .data[[e_col]] - .data[[f_col]],
        lag_error = lag(.data[[e_col]] - .data[[f_col]]),
        const = 1
      ) %>%
      na.omit()

    if (nrow(data_cur) > 10) {
      res <- tryCatch({
        gmm(g = gmm_moments,
            x = data_cur,
            t0 = c(0.5),
            type = "cue",
            method = "Brent",
            lower = 0.01,
            upper = 0.99)
      }, error = function(e) NULL)

      if (!is.null(res)) {
        alpha_hat <- coef(res)
        vcv_hat <- vcov(res)
        se_alpha <- sqrt(vcv_hat[1, 1])
        t_stat <- (alpha_hat - 0.5) / se_alpha
        p_val <- 2 * pt(-abs(t_stat), df = nrow(data_cur) - 1)

        currency_results[[cur]] <- list(
          summary = summary(res),
          alpha = alpha_hat,
          vcov = vcv_hat,
          t_stat = t_stat,
          p_val = p_val
        )
      }
    } else {
      message("Not enough data for ", cur)
    }
  }

  return(currency_results)
}

estimate_a2 <- function(currencies, wide_data, p = 2) {
  gmm_moments <- function(theta, data) {
    alpha <- theta[1]
    e <- data$forecast_error
    z <- as.matrix(data[, c("const", "lag_error")])
    lambda <- alpha + (1 - 2 * alpha) * (e < 0)
    g <- lambda * e^(p - 1) * z
    return(g)
  }

  currency_results <- list()

  for (cur in currencies) {
    message("Running GMM for ", cur)

    s_col  <- paste0("spot_", cur)
    s_t1   <- paste0("spot_", cur, "_t1")
    f_col  <- paste0("forward_", cur)

    if (!all(c(s_col, s_t1, f_col) %in% names(wide_data))) {
      message("Missing data columns for ", cur)
      next
    }

    data_cur <- wide_data %>%
      transmute(
        forecast_error = .data[[s_t1]] - .data[[f_col]],     # (sₜ₊₁ − fₜ)
        lag_error = lag(.data[[s_t1]] - .data[[f_col]]),     # lagged forecast error
        const = 1
      ) %>%
      na.omit()

    if (nrow(data_cur) > 10) {
      res <- tryCatch({
        gmm(g = gmm_moments,
            x = data_cur,
            t0 = c(0.5),
            type = "cue",
            method = "Brent",
            lower = 0.01,
            upper = 0.99)
      }, error = function(e) {
        message("GMM failed for ", cur, ": ", e$message)
        return(NULL)
      })

      if (!is.null(res)) {
        alpha_hat <- coef(res)
        vcv_hat <- vcov(res)
        se_alpha <- sqrt(vcv_hat[1, 1])
        t_stat <- (alpha_hat - 0.5) / se_alpha
        p_val <- 2 * pt(-abs(t_stat), df = nrow(data_cur) - 1)

        currency_results[[cur]] <- list(
          summary = summary(res),
          alpha = alpha_hat,
          se = se_alpha,
          t = t_stat,
          p = p_val,
          n = nrow(data_cur)
        )
      }
    } else {
      message("Not enough data for ", cur)
    }
  }

  return(currency_results)
}


results2 <- estimate_a(currencies = currencies, wide_data = wide_data, p = 2)
result_m <- estimate_a2(currencies = currencies, wide_data = monthly_data, p = 2)


# View results for one example currency (e.g., EUR)
results2$EUR
results2$GBP
results2$CAD
results2$JPY


# Extract alpha estimates
alphas2 <- sapply(results2, function(x) if (!is.null(x)) x$alpha else NA)
print(alphas2)

# Extract p-values for H0: alpha = 0.5
p_vals2 <- sapply(results2, function(x) if (!is.null(x)) x$p_val else NA)
print(p_vals2)


```

# Porfolio Policy
Portfolio based on alpha, without the macro features yet. These will be added later. for now we are simply trying to create a strategy based on the alphas and we can see that they are being systematically under-/over-predicted based on the forward rates. This will alow us to optimise a portfolio allocation during 

```{r Rolling GMM Alpha Estimation}

plan(multisession, workers = parallel::detectCores() - 1)

roll_alpha_gmm_parallel <- function(data, currencies, window = 60, p = 2) {
  n <- nrow(data)
  date_seq <- data$YearMonth[(window + 1):n]
  inv_logit <- function(x) 1 / (1 + exp(-x))

  
  # Inner function for one currency
  estimate_alpha_for_currency <- function(cur) {
    e_col <- paste0(cur, "_excess")
    f_col <- paste0(cur, "_prem")
    
    alpha_vec <- rep(NA_real_, length(date_seq))
    
    for (i in seq_along(date_seq)) {
      idx_start <- i
      idx_end <- i + window - 1
      sub_data <- data[idx_start:idx_end, ]
      
      dat_cur <- sub_data %>%
        transmute(
          forecast_error = .data[[e_col]] - .data[[f_col]],
          lag_error = lag(.data[[e_col]] - .data[[f_col]]),
          const = 1
        ) %>% na.omit()
      
      
      if (nrow(dat_cur) > 10) {
        gmm_moments <- function(theta, data) {
          alpha <- theta[1]
          e <- data$forecast_error
          z <- matrix(1, nrow = length(e), ncol = 1) #z <- as.matrix(data[, c("const", "lag_error")])
          lambda <- (e < 0) - alpha
            g <- z * lambda * abs(e)^(p - 1)
          return(g)
        }
        
        res <- tryCatch({
  gmm(g = gmm_moments, x = dat_cur, t0 = c(0),  # t0 is on logit scale
      type = "cue", method = "BFGS")
}, error = function(e) NULL)
        
            alpha_vec[i] <- if (!is.null(res)) inv_logit(coef(res)[1]) else NA
      }
    }
    
    return(alpha_vec)
  }
  
  # Run in parallel
  alpha_list <- future_map(currencies, estimate_alpha_for_currency, .progress = TRUE)
  names(alpha_list) <- currencies
  
  alpha_matrix <- do.call(cbind, alpha_list)
  rownames(alpha_matrix) <- as.character(date_seq)
  return(alpha_matrix)
}

alpha_matrix <- roll_alpha_gmm_parallel(wide_month, currencies, window = 60, p = p)

```


Now that we have the alpha calculations, we can use these simply as weights for the portfolio of currencies. We will size the portfolio based on the alphas and how far they are from 0.5.

```{r Simple Portfolio Allocation}
alpha_long <- as.data.frame(alpha_matrix) %>%
  rownames_to_column("Date") %>%
  pivot_longer(-Date, names_to = "Currency", values_to = "Alpha") %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d"),
    Alpha = as.numeric(Alpha),
    Deviation = abs(Alpha - 0.5),                    # how asymmetric
    Direction = ifelse(Alpha < 0.5, "Long", "Short") # investment direction
  )

# creating the weights
alpha_long <- alpha_long %>%
  group_by(Date) %>%
  mutate(
    RawWeight = ifelse(Direction == "Long", Deviation, -Deviation),
    SumAbs = sum(abs(RawWeight), na.rm = TRUE),
    Weight = ifelse(SumAbs > 0, RawWeight / SumAbs, 0)
  ) %>%
  ungroup()

returns_long <- wide_month %>%
  mutate(Date = YearMonth) %>%
  dplyr::select(Date, ends_with("_excess")) %>%
  pivot_longer(-Date, names_to = "Currency", values_to = "ExcessReturn") %>%
  mutate(Currency = sub("_excess", "", Currency))

alpha_portfolio <- alpha_long %>%
  left_join(returns_long, by = c("Date", "Currency")) %>%
  group_by(Date) %>%
  summarise(PortfolioReturn = sum(Weight * ExcessReturn, na.rm = TRUE), .groups = "drop") %>%
    mutate(CumulativeReturn = cumprod(1 + PortfolioReturn) - 1)


```

```{r Plotting Simple Alpha Portfolio Returns}
ggplot(alpha_portfolio, aes(x = Date, y = CumulativeReturn)) +
  geom_line(color = "blue", size = 1) +
  labs(title = "Cumulative Returns of Alpha-Based Portfolio",
       x = "Date",
       y = "Cumulative Return") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

### Improved Approaches to the simple alpha-based strategy

```{r improved alpha-based strat}
# Function to apply different smoothing methods
apply_alpha_smoothing <- function(alpha_data, method = "ewma", lambda = 0.94, sma_window = 12) {
  alpha_data %>%
    group_by(Currency) %>%
    arrange(Date) %>%
    mutate(
      # Exponentially Weighted Moving Average (EWMA)
      # Based on RiskMetrics methodology (J.P. Morgan, 1996)
      Alpha_EWMA = case_when(
        method %in% c("ewma", "both") ~ {
          alpha_vec <- Alpha
          alpha_vec[1] <- Alpha[1]  # Initialize with first value
          for(i in 2:length(alpha_vec)) {
            if(!is.na(alpha_vec[i-1]) & !is.na(Alpha[i])) {
              alpha_vec[i] <- lambda * alpha_vec[i-1] + (1 - lambda) * Alpha[i]
            }
          }
          alpha_vec
        },
        TRUE ~ Alpha
      ),
      
      # Simple Moving Average (SMA)
      Alpha_SMA = case_when(
        method %in% c("sma", "both") ~ rollmean(Alpha, k = sma_window, fill = NA, align = "right"),
        TRUE ~ Alpha
      ),
      
      # Adaptive smoothing based on alpha volatility
      # Higher volatility = more smoothing (Moreira & Muir, 2017)
      Alpha_Volatility = roll_sd(Alpha, n = 12, fill = NA, align = "right"),
      Adaptive_Lambda = pmax(0.8, pmin(0.98, 0.94 + 0.04 * (Alpha_Volatility / max(Alpha_Volatility, na.rm = TRUE)))),
      
      Alpha_Adaptive = case_when(
        method %in% c("adaptive", "both") ~ {
          alpha_vec <- Alpha
          lambda_vec <- Adaptive_Lambda
          alpha_vec[1] <- Alpha[1]
          for(i in 2:length(alpha_vec)) {
            if(!is.na(alpha_vec[i-1]) & !is.na(Alpha[i]) & !is.na(lambda_vec[i])) {
              alpha_vec[i] <- lambda_vec[i] * alpha_vec[i-1] + (1 - lambda_vec[i]) * Alpha[i]
            }
          }
          alpha_vec
        },
        TRUE ~ Alpha
      ),
      
      # Select the smoothed alpha based on method
      Alpha_Smoothed = case_when(
        method == "ewma" ~ Alpha_EWMA,
        method == "sma" ~ Alpha_SMA,
        method == "adaptive" ~ Alpha_Adaptive,
        method == "both" ~ (Alpha_EWMA + Alpha_SMA) / 2,
        TRUE ~ Alpha
      )
    ) %>%
    ungroup()
}

# Enhanced weight calculation with volatility scaling and turnover control
calculate_enhanced_weights <- function(alpha_data, returns_data, 
                                     vol_target = 0.10, 
                                     turnover_penalty = 0.002,
                                     min_weight = 0.05) {
  
  # Calculate rolling volatility for each currency (Barroso & Santa-Clara, 2015)
  returns_vol <- returns_data %>%
    group_by(Currency) %>%
    arrange(Date) %>%
    mutate(
      RollingVol = roll_sd(ExcessReturn, n = 12, fill = NA, align = "right"),
      VolAdjReturn = ExcessReturn / pmax(RollingVol, 0.01)  # Volatility-adjusted returns
    ) %>%
    ungroup()
  
  # Enhanced weight calculation
  enhanced_weights <- alpha_data %>%
    left_join(returns_vol, by = c("Date", "Currency")) %>%
    group_by(Date) %>%
    mutate(
      # Original deviation-based signal
      Deviation = abs(Alpha_Smoothed - 0.5),
      Direction = ifelse(Alpha_Smoothed < 0.5, "Long", "Short"),
      
      # Volatility scaling (Moreira & Muir, 2017)
      # Scale positions inversely with volatility
      VolScale = pmin(2, vol_target / pmax(RollingVol, 0.01)),
      
      # Enhanced signal strength
      SignalStrength = Deviation * VolScale,
      
      # Raw weights with volatility adjustment
      RawWeight = ifelse(Direction == "Long", SignalStrength, -SignalStrength),
      
      # Apply minimum weight threshold to reduce noise
      RawWeight_Filtered = ifelse(abs(RawWeight) < min_weight, 0, RawWeight),
      
      # Normalize weights
      SumAbs = sum(abs(RawWeight_Filtered), na.rm = TRUE),
      Weight_Prelim = ifelse(SumAbs > 0, RawWeight_Filtered / SumAbs, 0)
    ) %>%
    ungroup() %>%
    
    # Turnover control: penalize large weight changes
    group_by(Currency) %>%
    arrange(Date) %>%
    mutate(
      Weight_Lag = lag(Weight_Prelim, default = 0),
      Weight_Change = abs(Weight_Prelim - Weight_Lag),
      
      # Apply turnover penalty (DeMiguel et al., 2009)
      Turnover_Penalty = ifelse(Weight_Change > turnover_penalty, 
                               Weight_Change * 0.5, 0),
      
      # Adjust weights for turnover
      Weight_Adjusted = Weight_Prelim * (1 - Turnover_Penalty)
    ) %>%
    ungroup() %>%
    
    # Final normalization
    group_by(Date) %>%
    mutate(
      SumAbs_Final = sum(abs(Weight_Adjusted), na.rm = TRUE),
      Weight = ifelse(SumAbs_Final > 0, Weight_Adjusted / SumAbs_Final, 0)
    ) %>%
    ungroup()
  
  return(enhanced_weights)
}

# Helper function to detect data structure and extract returns
detect_and_extract_returns <- function(data) {
  # Check if data has YearMonth column (assumed to be date column)
  date_col <- NULL
  if ("YearMonth" %in% names(data)) {
    date_col <- "YearMonth"
  } else if ("Date" %in% names(data)) {
    date_col <- "Date"
  } else {
    # Look for date-like columns
    date_candidates <- names(data)[sapply(data, function(x) inherits(x, c("Date", "POSIXct", "POSIXlt")))]
    if (length(date_candidates) > 0) {
      date_col <- date_candidates[1]
      warning(paste("Using", date_col, "as date column"))
    } else {
      stop("No date column found. Please ensure your data has a Date or YearMonth column.")
    }
  }
  
  # Look for excess return columns (priority order)
  excess_cols <- names(data)[grepl("_excess$", names(data))]
  
  if (length(excess_cols) == 0) {
    # Look for return columns
    return_cols <- names(data)[grepl("_return$|_ret$", names(data), ignore.case = TRUE)]
    
    if (length(return_cols) == 0) {
      # Try to construct excess returns from spot and forward rates
      spot_cols <- names(data)[grepl("^spot_", names(data))]
      forward_cols <- names(data)[grepl("^forward_", names(data))]
      
      if (length(spot_cols) > 0 && length(forward_cols) > 0) {
        # Extract currency codes
        spot_currencies <- gsub("^spot_", "", spot_cols)
        forward_currencies <- gsub("^forward_", "", forward_cols)
        common_currencies <- intersect(spot_currencies, forward_currencies)
        
        if (length(common_currencies) > 0) {
          cat("Constructing excess returns from spot and forward rates for currencies:", 
              paste(common_currencies, collapse = ", "), "\n")
          
          # Construct excess returns: spot_t+1 - forward_t
          for (curr in common_currencies) {
            spot_col <- paste0("spot_", curr)
            forward_col <- paste0("forward_", curr)
            spot_t1_col <- paste0("spot_", curr, "_t1")
            
            if (spot_t1_col %in% names(data)) {
              # Use provided t+1 spot rates
              data[[paste0(curr, "_excess")]] <- data[[spot_t1_col]] - data[[forward_col]]
            } else {
              # Use lead of spot rates
              data[[paste0(curr, "_excess")]] <- lead(data[[spot_col]]) - data[[forward_col]]
            }
          }
          excess_cols <- paste0(common_currencies, "_excess")
        } else {
          stop("Cannot identify matching spot and forward rate columns to construct excess returns")
        }
      } else {
        stop("No excess return columns found and cannot construct them from available data")
      }
    } else {
      excess_cols <- return_cols
      cat("Using return columns:", paste(return_cols, collapse = ", "), "\n")
    }
  }
  
  # Create returns dataset
  returns_data <- data %>%
    dplyr::select(all_of(c(date_col, excess_cols))) %>%
    rename(Date = !!date_col) %>%
    mutate(Date = as.Date(Date))
  
  return(list(
    returns_data = returns_data,
    currencies = gsub("_excess$|_return$|_ret$", "", excess_cols, ignore.case = TRUE),
    excess_cols = excess_cols
  ))
}

# Main enhanced portfolio allocation function
enhanced_alpha_portfolio_allocation <- function(alpha_matrix, returns_data, 
                                              smoothing_method = "ewma",
                                              lambda = 0.94,
                                              sma_window = 12,
                                              vol_target = 0.10,
                                              turnover_penalty = 0.002,
                                              min_weight = 0.05) {
  
  # Convert alpha matrix to long format
  alpha_long <- as.data.frame(alpha_matrix) %>%
    rownames_to_column("Date") %>%
    pivot_longer(-Date, names_to = "Currency", values_to = "Alpha") %>%
    mutate(
      Date = as.Date(Date, format = "%Y-%m-%d"),
      Alpha = as.numeric(Alpha)
    ) %>%
    filter(!is.na(Alpha))
  
  # Apply smoothing
  alpha_smoothed <- apply_alpha_smoothing(alpha_long, 
                                        method = smoothing_method,
                                        lambda = lambda,
                                        sma_window = sma_window)
  
  # Detect and extract returns data
  returns_info <- detect_and_extract_returns(returns_data)
  
  # Prepare returns data in long format
  returns_long <- returns_info$returns_data %>%
    pivot_longer(-Date, names_to = "Currency", values_to = "ExcessReturn") %>%
    mutate(Currency = gsub("_excess$|_return$|_ret$", "", Currency, ignore.case = TRUE))
  
  # Calculate enhanced weights
  portfolio_weights <- calculate_enhanced_weights(alpha_smoothed, returns_long,
                                                vol_target = vol_target,
                                                turnover_penalty = turnover_penalty,
                                                min_weight = min_weight)
  
  # Calculate portfolio returns with transaction costs
  alpha_portfolio <- portfolio_weights %>%
    left_join(returns_long, by = c("Date", "Currency")) %>%
    group_by(Date) %>%
    summarise(
      # Portfolio return before transaction costs
      PortfolioReturn_Gross = sum(Weight * ExcessReturn, na.rm = TRUE),
      
      # Transaction costs (simple model: 0.1% per unit of turnover)
      TotalTurnover = sum(abs(Weight_Change), na.rm = TRUE) / 2,
      TransactionCosts = TotalTurnover * 0.001,
      
      # Net portfolio return
      PortfolioReturn = PortfolioReturn_Gross - TransactionCosts,
      
      # Additional metrics
      ActivePositions = sum(abs(Weight) > 0.01, na.rm = TRUE),
      PortfolioVolatility = sqrt(sum((Weight * ExcessReturn)^2, na.rm = TRUE)),
      
      .groups = "drop"
    ) %>%
    arrange(Date) %>%
    mutate(
      CumulativeReturn = cumprod(1 + PortfolioReturn) - 1,
      CumulativeReturn_Gross = cumprod(1 + PortfolioReturn_Gross) - 1,
      
      # Rolling Sharpe ratio (annualized)
      RollingReturn = roll_mean(PortfolioReturn, n = 12, fill = NA, align = "right"),
      RollingVol = roll_sd(PortfolioReturn, n = 12, fill = NA, align = "right"),
      RollingSharpe = (RollingReturn * 12) / (RollingVol * sqrt(12))
    )
  
  # Create summary statistics
  summary_stats <- alpha_portfolio %>%
    filter(!is.na(PortfolioReturn)) %>%
    summarise(
      TotalReturn = tail(CumulativeReturn, 1),
      AnnualizedReturn = (1 + TotalReturn)^(12/n()) - 1,
      AnnualizedVol = sd(PortfolioReturn, na.rm = TRUE) * sqrt(12),
      SharpeRatio = AnnualizedReturn / AnnualizedVol,
      MaxDrawdown = max(cummax(CumulativeReturn) - CumulativeReturn, na.rm = TRUE),
      AvgTurnover = mean(TotalTurnover, na.rm = TRUE),
      AvgTransactionCosts = mean(TransactionCosts, na.rm = TRUE) * 12,
      .groups = "drop"
    )
  
  return(list(
    portfolio_returns = alpha_portfolio,
    portfolio_weights = portfolio_weights,
    summary_stats = summary_stats
  ))
}

# Example usage with different smoothing methods
# For your specific data structure:

# Method 1: EWMA Smoothing (λ = 0.94, typical for monthly data)
results_ewma <- enhanced_alpha_portfolio_allocation(
  alpha_matrix = alpha_matrix,
  returns_data = wide_month,  # Function will auto-detect structure
  smoothing_method = "ewma",
  lambda = 0.94,
  vol_target = 0.10,
  turnover_penalty = 0.002
)

# Alternative: If you already have excess returns calculated
# wide_month_with_excess <- wide_month %>%
#   mutate(
#     CAD_excess = spot_CAD_t1 - forward_CAD,
#     EUR_excess = spot_EUR_t1 - forward_EUR,
#     GBP_excess = spot_GBP_t1 - forward_GBP,
#     JPY_excess = spot_JPY_t1 - forward_JPY
#   )
# 
# results_ewma <- enhanced_alpha_portfolio_allocation(
#   alpha_matrix = alpha_matrix,
#   returns_data = wide_month_with_excess,
#   smoothing_method = "ewma",
#   lambda = 0.94
# )

# Method 2: Simple Moving Average (12-month window)
# results_sma <- enhanced_alpha_portfolio_allocation(
#   alpha_matrix = alpha_matrix,
#   returns_data = wide_month,
#   smoothing_method = "sma",
#   sma_window = 12,
#   vol_target = 0.10,
#   turnover_penalty = 0.002
# )

# Method 3: Adaptive smoothing (volatility-dependent)
# results_adaptive <- enhanced_alpha_portfolio_allocation(
#   alpha_matrix = alpha_matrix,
#   returns_data = wide_month,
#   smoothing_method = "adaptive",
#   vol_target = 0.10,
#   turnover_penalty = 0.001
# )

# Method 4: Combined EWMA + SMA
# results_combined <- enhanced_alpha_portfolio_allocation(
#   alpha_matrix = alpha_matrix,
#   returns_data = wide_month,
#   smoothing_method = "both",
#   lambda = 0.94,
#   sma_window = 12,
#   vol_target = 0.10,
#   turnover_penalty = 0.002
# )

# Performance comparison function
compare_strategies <- function(results_list) {
  comparison <- map_dfr(results_list, ~.x$summary_stats, .id = "Strategy")
  
  cat("Strategy Performance Comparison:\n")
  print(comparison)
  
  return(comparison)
}

# Usage example:
# results <- list(
#   "Original" = list(summary_stats = original_stats),
#   "EWMA" = results_ewma,
#   "SMA" = results_sma,
#   "Adaptive" = results_adaptive,
#   "Combined" = results_combined
# )
# 
# comparison <- compare_strategies(results)

# Visualization function for smoothed alphas
plot_alpha_smoothing <- function(alpha_data, currency = "GBP") {
  alpha_data %>%
    filter(Currency == currency) %>%
    select(Date, Alpha, Alpha_EWMA, Alpha_SMA, Alpha_Adaptive) %>%
    pivot_longer(-Date, names_to = "Method", values_to = "Alpha_Value") %>%
    ggplot(aes(x = Date, y = Alpha_Value, color = Method)) +
    geom_line() +
    geom_hline(yintercept = 0.5, linetype = "dashed", color = "black") +
    labs(title = paste("Alpha Smoothing Comparison:", currency),
         x = "Date", y = "Alpha Value") +
    theme_minimal()
}
```

```{r Smoothing alpha SMA / EWMA}
smooth_alpha <- as.data.frame(alpha_matrix) %>%
    rownames_to_column("Date") %>%
    mutate(Date = as.Date(Date))

alpha_ewma <- smooth_alpha %>%
  mutate(across(-Date, ~ EMA(., n = 10)))
alpha_sma <- smooth_alpha %>%
  mutate(across(-Date, ~ rollmean(., k = 10, fill = NA, align = "right")))


## Simple plot of the smoothed alphas

ggplot(alpha_ewma, aes(x = Date)) +
  geom_line(aes(y = GBP, color = "GBP")) +
  geom_line(aes(y = EUR, color = "EUR")) +
  geom_line(aes(y = JPY, color = "JPY")) +
  geom_line(aes(y = CAD, color = "CAD")) +
  labs(title = "Smoothed Alphas (EWMA)",
       x = "Date",
       y = "Alpha") +
  theme_minimal() +
  scale_color_manual(values = c("GBP" = "blue", 
                                 "EUR" = "red", 
                                 "JPY" = "green", 
                                 "CAD" = "purple"))

ggplot(alpha_sma, aes(x = Date)) +
  geom_line(aes(y = GBP, color = "GBP")) +
  geom_line(aes(y = EUR, color = "EUR")) +
  geom_line(aes(y = JPY, color = "JPY")) +
  geom_line(aes(y = CAD, color = "CAD")) +
  labs(title = "Smoothed Alphas (SMA)",
       x = "Date",
       y = "Alpha") +
  theme_minimal() +
  scale_color_manual(values = c("GBP" = "blue", 
                                 "EUR" = "red", 
                                 "JPY" = "green", 
                                 "CAD" = "purple"))
```

```{r Implementing the alpha strat on the smoothed data}
# Implementing the alpha strategy on the smoothed data
alpha_sma_long <- alpha_sma %>%
    pivot_longer(-Date, names_to = "Currency", values_to = "Alpha") %>%
  mutate(Alpha = as.numeric(Alpha),
    Deviation = abs(Alpha - 0.5),                    # how asymmetric
    Direction = ifelse(Alpha < 0.5, "Long", "Short") # investment direction
  ) %>%
    group_by(Date) %>%
  mutate(
    RawWeight = ifelse(Direction == "Long", Deviation, -Deviation),
    SumAbs = sum(abs(RawWeight), na.rm = TRUE),
    Weight = ifelse(SumAbs > 0, RawWeight / SumAbs, 0)
  ) %>%
  ungroup()

alpha_ewma_long <- alpha_ewma %>%
    pivot_longer(-Date, names_to = "Currency", values_to = "Alpha") %>%
  mutate(Alpha = as.numeric(Alpha),
    Deviation = abs(Alpha - 0.5),                    # how asymmetric
    Direction = ifelse(Alpha < 0.5, "Long", "Short") # investment direction
  ) %>%
    group_by(Date) %>%
  mutate(
    RawWeight = ifelse(Direction == "Long", Deviation, -Deviation),
    SumAbs = sum(abs(RawWeight), na.rm = TRUE),
    Weight = ifelse(SumAbs > 0, RawWeight / SumAbs, 0)
  ) %>%
  ungroup()


a_ewma_portfolio <- alpha_ewma_long %>%
  left_join(returns_long, by = c("Date", "Currency")) %>%
  group_by(Date) %>%
  summarise(PortfolioReturn = sum(Weight * ExcessReturn, na.rm = TRUE), .groups = "drop") %>%
    mutate(CumulativeReturn = cumprod(1 + PortfolioReturn) - 1)
a_sma_portfolio <- alpha_sma_long %>%
  left_join(returns_long, by = c("Date", "Currency")) %>%
  group_by(Date) %>%
  summarise(PortfolioReturn = sum(Weight * ExcessReturn, na.rm = TRUE), .groups = "drop") %>%
    mutate(CumulativeReturn = cumprod(1 + PortfolioReturn) - 1)

a_portfolio <- alpha_portfolio %>%
  rename(Original_CumulativeReturn = CumulativeReturn) %>%
  left_join(a_ewma_portfolio %>% rename(EWMA_CumulativeReturn = CumulativeReturn), by = "Date") %>%
  left_join(a_sma_portfolio %>% rename(SMA_CumulativeReturn = CumulativeReturn), by = "Date")
    

## plotting returns
ggplot(a_portfolio, aes(x = Date)) +
  geom_line(aes(y = EWMA_CumulativeReturn, color = "EWMA")) +
  geom_line(aes(y = SMA_CumulativeReturn, color = "SMA")) +
  geom_line(aes(y = Original_CumulativeReturn, color = "No Smoothing")) +
  labs(title = "Cumulative Returns of Smoothed Alpha Portfolios",
       x = "Date",
       y = "Cumulative Return") +
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())




```







# D-PPP
Given we have these alphas that we are estimating we can see that based on the forward rates, some currencies are systematically under-/over- predicted. We aim to utilise this fact and optimise portfolio allocation based on these utiltiy asymmetries calculated on a rolling basis.

```{r Sim data}
currs <- c("GBP", "EUR", "JPY", "CAD")
n <- nrow(wide_data)

simulate_macro <- function(base, vol, n) {
  rnorm(n, mean = base, sd = vol)
}

wide_data2 <- wide_data %>%
  mutate(
    int_US = simulate_macro(0.015, 0.002, n),
    inf_US = simulate_macro(0.02,  0.002, n),
    gdp_US = simulate_macro(0.025, 0.003, n)
  )

# Simulate for each currency and compute differentials
for (ccy in currencies) {
  wide_data2[[paste0("int_", ccy)]] <- simulate_macro(0.02, 0.0025, n)
  wide_data2[[paste0("inf_", ccy)]] <- simulate_macro(0.025, 0.0025, n)
  wide_data2[[paste0("gdp_", ccy)]] <- simulate_macro(0.03, 0.003, n)
  
  # Compute differentials
  wide_data2[[paste0("int_diff_", ccy)]] <- wide_data2[[paste0("int_", ccy)]] - wide_data2$int_US
  wide_data2[[paste0("inf_diff_", ccy)]] <- wide_data2[[paste0("inf_", ccy)]] - wide_data2$inf_US
  wide_data2[[paste0("gdp_diff_", ccy)]] <- wide_data2[[paste0("gdp_", ccy)]] - wide_data2$gdp_US
}

wide_month_macro <- wide_month %>%
  mutate(
    int_US = simulate_macro(0.015, 0.002, nrow(wide_month)),
    inf_US = simulate_macro(0.02,  0.002, nrow(wide_month)),
    gdp_US = simulate_macro(0.025, 0.003, nrow(wide_month))
  )

# Simulate for each currency and compute differentials
for (ccy in currencies) {
  wide_month_macro[[paste0("int_", ccy)]] <- simulate_macro(0.02, 0.0025, nrow(wide_month))
  wide_month_macro[[paste0("inf_", ccy)]] <- simulate_macro(0.025, 0.0025, nrow(wide_month))
  wide_month_macro[[paste0("gdp_", ccy)]] <- simulate_macro(0.03, 0.003, nrow(wide_month))
  
  # Compute differentials
  wide_month_macro[[paste0("int_diff_", ccy)]] <- wide_month_macro[[paste0("int_", ccy)]] - wide_month_macro$int_US
  wide_month_macro[[paste0("inf_diff_", ccy)]] <- wide_month_macro[[paste0("inf_", ccy)]] - wide_month_macro$inf_US
  wide_month_macro[[paste0("gdp_diff_", ccy)]] <- wide_month_macro[[paste0("gdp_", ccy)]] - wide_month_macro$gdp_US
}

head(wide_month_macro)

```


```{r D-PPP Trading Strategy Daily Data}
library(keras)
library(tensorflow)


# -------- 0. User inputs --------
# wide_data: Must contain Date, for each currency: XX_excess, XX_FWD, and macro features
# currencies: e.g. c("GBP", "EUR", "JPY", "CAD")
# window: size of rolling GMM window (in months)
# p: power for loss (1 = lin-lin, 2 = quad-quad)
window <- 1260  # 5 years daily
p <- 2         # quad-quad loss
macro_prefixes <- c() # (optional) if you want to filter macro features by prefix, e.g. c("infl_", "gdp_")

# -------- 1. Identify Macro Features Automatically --------
exclude_patterns <- c("_excess$", "_FWD$", "^Date$", "^date$", "^const$", "^lag_")
macro_features <- names(wide_data)
for (pat in exclude_patterns) {
  macro_features <- macro_features[!grepl(pat, macro_features)]
}
if (length(macro_prefixes) > 0) {
  macro_features <- macro_features[grepl(paste(macro_prefixes, collapse="|"), macro_features)]
}
# Remove any duplicated names just in case
macro_features <- unique(macro_features)

# -------- 2. Rolling GMM Alpha Estimation (Elliott et al. 2006) --------
library(furrr)
library(dplyr)
library(gmm)
plan(multisession, workers = parallel::detectCores() - 1)

roll_alpha_gmm_parallel <- function(data, currencies, window = 60, p = 2) {
  n <- nrow(data)
  date_seq <- data$Date[(window + 1):n]
  
  # Inner function for one currency
  estimate_alpha_for_currency <- function(cur) {
    e_col <- paste0(cur, "_excess")
    f_col <- paste0(cur, "_FWD")
    
    alpha_vec <- rep(NA_real_, length(date_seq))
    
    for (i in seq_along(date_seq)) {
      idx_start <- i
      idx_end <- i + window - 1
      sub_data <- data[idx_start:idx_end, ]
      
      dat_cur <- sub_data %>%
        transmute(
          forecast_error = .data[[e_col]] - .data[[f_col]],
          lag_error = lag(.data[[e_col]] - .data[[f_col]]),
          const = 1
        ) %>% na.omit()
      
      if (nrow(dat_cur) > 10) {
        gmm_moments <- function(theta, data) {
          alpha <- theta[1]
          e <- data$forecast_error
          z <- as.matrix(data[, c("const", "lag_error")])
          lambda <- alpha + (1 - 2 * alpha) * (e < 0)
          g <- lambda * e^(p - 1) * z
          return(g)
        }
        
        res <- tryCatch({
          gmm(g = gmm_moments, x = dat_cur, t0 = c(0.5),
              type = "cue", method = "Brent", lower = 0.01, upper = 0.99)
        }, error = function(e) NULL)
        
        alpha_vec[i] <- if (!is.null(res)) coef(res)[1] else NA
      }
    }
    
    return(alpha_vec)
  }
  
  # Run in parallel
  alpha_list <- future_map(currencies, estimate_alpha_for_currency, .progress = TRUE)
  names(alpha_list) <- currencies
  
  alpha_matrix <- do.call(cbind, alpha_list)
  rownames(alpha_matrix) <- as.character(date_seq)
  return(alpha_matrix)
}



alpha_matrix <- roll_alpha_gmm_parallel(wide_data, currencies, window = window, p = p)
 date_seq <- as.Date(rownames(alpha_matrix))
n_steps <- length(date_seq)
N_assets <- length(currencies)

# -------- 3. Build Features and Target Arrays for Neural Net --------
F_feats <- length(macro_features) + 2  # fwd prem + alpha + all macro features

X_feats <- array(NA_real_, dim = c(n_steps, N_assets, F_feats),
                 dimnames = list(time = as.character(date_seq), asset = currencies,
                                 feature = c("fwd_prem", macro_features, "alpha")))
R_next  <- matrix(NA_real_, nrow = n_steps, ncol = N_assets, dimnames = list(as.character(date_seq), currencies))

for (i in seq_len(n_steps)) {
  t <- date_seq[i]
  t1 <- t %m+% months(1)
  row <- wide_data %>% filter(Date == t)
  next_row <- wide_data %>% filter(Date == t1)
  for (j in seq_along(currencies)) {
    cur <- currencies[j]
    fwd_prem <- if (!is.null(row[[paste0(cur, "_FWD")]]) & !is.null(row[[paste0(cur, "_excess")]])) {
      row[[paste0(cur, "_FWD")]] - row[[paste0(cur, "_excess")]]
    } else { NA_real_ }
    macro_vals <- as.numeric(row[1, macro_features, drop = FALSE])
    alpha_val <- alpha_matrix[i, cur]
    X_feats[i, j, ] <- c(fwd_prem, macro_vals, alpha_val)
    R_next[i, j] <- if (nrow(next_row) > 0) next_row[[paste0(cur, "_excess")]] else NA
  }
}

# Remove rows where all R_next are NA (no returns to predict)
valid_idx <- which(!apply(is.na(R_next), 1, all))
X_feats_final     <- X_feats[valid_idx, , , drop = FALSE]
R_next_final      <- R_next[valid_idx, , drop = FALSE]
train_dates_final <- date_seq[valid_idx]

# Optional: Remove rows where ANY macro feature is NA
good_rows <- sapply(seq_len(dim(X_feats_final)[1]), function(i) !any(is.na(X_feats_final[i, , ])))
X_feats_final     <- X_feats_final[good_rows, , , drop = FALSE]
R_next_final      <- R_next_final[good_rows, , drop = FALSE]
train_dates_final <- train_dates_final[good_rows]

# -------- 4. Keras Deep PPP Model Definition --------
F_feats <- dim(X_feats_final)[3]
N_assets <- dim(X_feats_final)[2]

asset_net <- keras_model_sequential(name = 'asset_net') %>%
  layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-4), input_shape = c(F_feats)) %>%
  layer_activation_leaky_relu(alpha = 0.01) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(1e-4)) %>%
  layer_activation_leaky_relu(alpha = 0.01) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = 'linear', kernel_regularizer = regularizer_l2(1e-4))

input_layer <- layer_input(shape = c(N_assets, F_feats), name = 'features')
scores      <- input_layer %>% time_distributed(asset_net)
scores_flat <- scores %>% layer_flatten()
g_avg       <- scores_flat %>% layer_lambda(function(x) k_mean(x, axis = 1L, keepdims = TRUE))
g_centered  <- list(scores_flat, g_avg) %>%
  layer_lambda(function(lst) lst[[1]] - lst[[2]])
one_over_N  <- 1.0 / as.numeric(N_assets)
weights_out <- g_centered %>%
  layer_lambda(function(x) x * one_over_N + one_over_N, name = 'weights')
model <- keras_model(inputs = input_layer, outputs = weights_out)

rr <- 5
cap_percent <- 0.03
lambda_max  <- 50
dcrra_loss <- function(y_true, y_pred) {
  r_p       <- k_sum(y_pred * y_true, axis = 1L)
  U         <- k_pow(1 + r_p, 1 - rr) / (1 - rr)
  base_loss <- -k_mean(U)
  excess_pos <- k_relu(y_pred - cap_percent)
  excess_neg <- k_relu(-cap_percent - y_pred)
  penalty    <- k_mean(k_sum(k_square(excess_pos) + k_square(excess_neg), axis = 1L))
  base_loss + lambda_max * penalty
}
model %>% compile(loss = dcrra_loss, optimizer = optimizer_adam(learning_rate = 5e-5))

# -------- 5. Model Training --------
early_stop <- callback_early_stopping(monitor = 'val_loss', patience = 7, restore_best_weights = TRUE)
history <- model %>% fit(
  x                = X_feats_final,
  y                = R_next_final,
  epochs           = 200,
  batch_size       = 32,
  validation_split = 0.15,
  callbacks        = list(early_stop),
  verbose          = 2
)

# -------- 6. Extract Portfolio Returns & Weights --------
pred_weights_final <- predict(model, X_feats_final)
ppp_returns_final  <- rowSums(pred_weights_final * R_next_final)
cum_ppp_final      <- cumprod(1 + ppp_returns_final) - 1

# (Optional) convert to xts
ppp_returns_xts_raw <- xts(ppp_returns_final, order.by = as.Date(train_dates_final))
# To get monthly portfolio weights:
weights_xts <- xts(pred_weights_final, order.by = as.Date(train_dates_final))
colnames(weights_xts) <- currencies

# -------- 7. (Optional) Benchmark: Naive Forward-Rate Strategy --------
# E.g. equally weighted or carry (long highest forward premium)
naive_weights <- matrix(1/N_assets, nrow = nrow(R_next_final), ncol = N_assets)
naive_returns <- rowSums(naive_weights * R_next_final)
naive_xts <- xts(naive_returns, order.by = as.Date(train_dates_final))

# -------- 8. Plot Results --------
library(ggplot2)
library(tidyr)
cum_df <- data.frame(
  Date = as.Date(train_dates_final),
  D_PPP = cumprod(1 + ppp_returns_final),
  Naive = cumprod(1 + naive_returns)
) %>% pivot_longer(-Date, names_to = "Strategy", values_to = "Cumulative_Return")

ggplot(cum_df, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line() + theme_minimal() +
  labs(title = "Cumulative Return: D-PPP vs Naive", y = "Growth of $1", x = "Date")

# --------- END OF SCRIPT ----------
```

```{r D-PPP Trading Strategy Monthly Data}
# -------- 0. User inputs --------
# wide_data: Must contain Date, for each currency: XX_excess, XX_FWD, and macro features
# currencies: e.g. c("GBP", "EUR", "JPY", "CAD")
# window: size of rolling GMM window (in months)
# p: power for loss (1 = lin-lin, 2 = quad-quad)
window_m <- 60  # 5 years daily
p <- 2         # quad-quad loss
macro_prefixes <- c() # (optional) if you want to filter macro features by prefix, e.g. c("infl_", "gdp_")

# -------- 1. Identify Macro Features Automatically --------
exclude_patterns <- c("_excess$", "_FWD$", "^Date$", "^date$", "^const$", "^lag_")
macro_features <- names(wide_month_macro)
for (pat in exclude_patterns) {
  macro_features <- macro_features[!grepl(pat, macro_features)]
}
if (length(macro_prefixes) > 0) {
  macro_features <- macro_features[grepl(paste(macro_prefixes, collapse="|"), macro_features)]
}
# Remove any duplicated names just in case
macro_features <- unique(macro_features)



 date_seq <- as.Date(rownames(alpha_matrix))
n_steps <- length(date_seq)
N_assets <- length(currencies)

# -------- 3. Build Features and Target Arrays for Neural Net --------
F_feats <- length(macro_features) + 2  # fwd prem + alpha + all macro features

X_feats <- array(NA_real_, dim = c(n_steps, N_assets, F_feats),
                 dimnames = list(time = as.character(date_seq), asset = currencies,
                                 feature = c("fwd_prem", macro_features, "alpha")))
R_next  <- matrix(NA_real_, nrow = n_steps, ncol = N_assets, dimnames = list(as.character(date_seq), currencies))

for (i in seq_len(n_steps)) {
  t <- date_seq[i]
  t1 <- t %m+% months(1)
  row <- wide_month_macro %>% dplyr::filter(YearMonth == t)
  next_row <- wide_month_macro %>% filter(YearMonth == t1)
  for (j in seq_along(currencies)) {
    cur <- currencies[j]
    fwd_prem <- if (!is.null(row[[paste0(cur, "_prem")]]) & !is.null(row[[paste0(cur, "_excess")]])) {
      row[[paste0(cur, "_prem")]] - row[[paste0(cur, "_excess")]]
    } else { NA_real_ }
    macro_vals <- as.numeric(row[1, macro_features, drop = FALSE])
    alpha_val <- alpha_matrix[i, cur]
    X_feats[i, j, ] <- c(fwd_prem, macro_vals, alpha_val)
    R_next[i, j] <- if (nrow(next_row) > 0) next_row[[paste0(cur, "_excess")]] else NA
  }
}

# Remove rows where all R_next are NA (no returns to predict)
valid_idx <- which(!apply(is.na(R_next), 1, all))
X_feats_final     <- X_feats[valid_idx, , , drop = FALSE]
R_next_final      <- R_next[valid_idx, , drop = FALSE]
train_dates_final <- date_seq[valid_idx]

# Optional: Remove rows where ANY macro feature is NA
good_rows <- sapply(seq_len(dim(X_feats_final)[1]), function(i) !any(is.na(X_feats_final[i, , ])))
X_feats_final     <- X_feats_final[good_rows, , , drop = FALSE]
R_next_final      <- R_next_final[good_rows, , drop = FALSE]
train_dates_final <- train_dates_final[good_rows]

# -------- 4. Keras Deep PPP Model Definition --------
F_feats <- dim(X_feats_final)[3]
N_assets <- dim(X_feats_final)[2]

reticulate::use_virtualenv("r-keras", required = TRUE)
library(keras)
library(reticulate)

asset_net <- keras_model_sequential(name = 'asset_net')


asset_net <- keras_model_sequential(name = 'asset_net') %>%
  layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-4), input_shape = c(F_feats)) %>%
  layer_activation_leaky_relu(alpha = 0.01) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(1e-4)) %>%
  layer_activation_leaky_relu(alpha = 0.01) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = 'linear', kernel_regularizer = regularizer_l2(1e-4))

input_layer <- layer_input(shape = c(N_assets, F_feats), name = 'features')
scores      <- input_layer %>% time_distributed(asset_net)
scores_flat <- scores %>% layer_flatten()

g_avg       <- scores_flat %>% layer_lambda(function(x) k_mean(x, axis = 1L, keepdims = TRUE))

g_centered  <- list(scores_flat, g_avg) %>%
  layer_lambda(function(lst) lst[[1]] - lst[[2]])

one_over_N  <- 1.0 / as.numeric(N_assets)

weights_out <- g_centered %>%
  layer_lambda(function(x) x * one_over_N + one_over_N, name = 'weights')
model <- keras_model(inputs = input_layer, outputs = weights_out)

rr <- 5
cap_percent <- 0.03
lambda_max  <- 50
dcrra_loss <- function(y_true, y_pred) {
  r_p       <- k_sum(y_pred * y_true, axis = 1L)
  U         <- k_pow(1 + r_p, 1 - rr) / (1 - rr)
  base_loss <- -k_mean(U)
  excess_pos <- k_relu(y_pred - cap_percent)
  excess_neg <- k_relu(-cap_percent - y_pred)
  penalty    <- k_mean(k_sum(k_square(excess_pos) + k_square(excess_neg), axis = 1L))
  base_loss + lambda_max * penalty
}
model %>% compile(loss = dcrra_loss, optimizer = optimizer_adam(learning_rate = 5e-5))

# -------- 5. Model Training --------
early_stop <- callback_early_stopping(monitor = 'val_loss', patience = 7, restore_best_weights = TRUE)
history <- model %>% fit(
  x                = X_feats_final,
  y                = R_next_final,
  epochs           = 200,
  batch_size       = 32,
  validation_split = 0.15,
  callbacks        = list(early_stop),
  verbose          = 2
)

# -------- 6. Extract Portfolio Returns & Weights --------
pred_weights_final <- predict(model, X_feats_final)
ppp_returns_final  <- rowSums(pred_weights_final * R_next_final)
cum_ppp_final      <- cumprod(1 + ppp_returns_final) - 1

# (Optional) convert to xts
ppp_returns_xts_raw <- xts(ppp_returns_final, order.by = as.Date(train_dates_final))
# To get monthly portfolio weights:
weights_xts <- xts(pred_weights_final, order.by = as.Date(train_dates_final))
colnames(weights_xts) <- currencies

# -------- 7. (Optional) Benchmark: Naive Forward-Rate Strategy --------
# E.g. equally weighted or carry (long highest forward premium)
naive_weights <- matrix(1/N_assets, nrow = nrow(R_next_final), ncol = N_assets)
naive_returns <- rowSums(naive_weights * R_next_final)
naive_xts <- xts(naive_returns, order.by = as.Date(train_dates_final))

# -------- 8. Plot Results --------
library(ggplot2)
library(tidyr)
cum_df <- data.frame(
  Date = as.Date(train_dates_final),
  D_PPP = cumprod(1 + ppp_returns_final)
) %>% pivot_longer(-Date, names_to = "Strategy", values_to = "Cumulative_Return")

ggplot(cum_df, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line() + theme_minimal() +
  labs(title = "Cumulative Return: D-PPP vs Naive", y = "Growth of $1", x = "Date")

# --------- END OF SCRIPT ----------
```

```{r Plot Alphas}
alpha_df <- as.data.frame(alpha_matrix) %>%
  rownames_to_column("Date") %>%
  mutate(Date = as.Date(Date)) %>%
  pivot_longer(-Date, names_to = "Currency", values_to = "Alpha")

# Plot
ggplot(alpha_df, aes(x = Date, y = Alpha, color = Currency)) +
  geom_line() +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey") +
  theme_minimal() +
  labs(title = "Rolling GMM Alpha Estimates Over Time",
       y = expression(hat(alpha)),
       x = "Date")

```



# Expectiles/Quantiles/Transformed Data


```{r transformed excess returns lm}
# Step 1: Clean alpha names
alphas_clean <- setNames(as.numeric(alphas2), gsub("\\.Theta\\[1\\]", "", names(alphas2)))

# Step 2: Transform excess returns using loss function
wide_transformed <- wide_data

for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  trans_var  <- paste0(cur, "_excess_trans")

  if (cur %in% names(alphas_clean)) {
    alpha <- alphas_clean[[cur]]

    e <- wide_transformed[[excess_var]]
    wide_transformed[[trans_var]] <- (alpha + (1 - 2 * alpha) * (e < 0)) * abs(e)^2
  } else {
    wide_transformed[[trans_var]] <- NA_real_
  }
}

simple2 <- purrr::map_dfr(currencies, function(cur) {
  trans_var <- paste0(cur, "_excess_trans")
  prem_var  <- paste0(cur, "_prem")

  df <- wide_transformed %>%
    dplyr::select(all_of(c(trans_var, prem_var))) %>%
    filter(!is.na(.data[[trans_var]]), !is.na(.data[[prem_var]]))

  model <- lm(as.formula(paste0(trans_var, " ~ ", prem_var)), data = df)
  tidy(model) %>% mutate(Currency = cur)
})

# Step 4: Print results with hypothesis tests
for (cur in currencies) {
  cat("--------------", cur , "--------------\n")

  trans_var <- paste0(cur, "_excess_trans")
  prem_var  <- paste0(cur, "_prem")

  df <- wide_transformed %>%
    dplyr::select(all_of(c(trans_var, prem_var))) %>%
    filter(!is.na(.data[[trans_var]]), !is.na(.data[[prem_var]]))

  m <- lm(as.formula(paste0(trans_var, " ~ ", prem_var)), data = df)

  cat("  alpha:", round(coef(m)[1], 4), 
      " | beta:", round(coef(m)[2], 4), "\n")

  print(summary(m))
}

```

```{r Expectile Regression Function}
expectile_regression <- function(X, y, tau = 0.5) {
  X <- as.matrix(X)
  y <- as.numeric(y)
  X <- cbind(1, X)  
  n <- nrow(X)
  p <- ncol(X)
  
  # Loss function - quad-quad loss function.
  
  loss_fn <- function(beta) {
    y_hat <- X %*% beta
    e <- y - y_hat
    weights <- ifelse(e < 0, 1 - tau, tau)
    mean(weights * e^2)
  }
  
  # Gradient function
  grad_fn <- function(beta) {
    y_hat <- X %*% beta
    e <- y - y_hat
    weights <- ifelse(e < 0, 1 - tau, tau)
    grad <- -2 * t(X) %*% (weights * e) / n
    return(as.vector(grad))
  }
  
  # Optimize
  opt <- optim(
    par = rep(0, p),
    fn = loss_fn,
    gr = grad_fn,
    method = "BFGS",
    control = list(maxit = 500, reltol = 1e-8)
  )
  
  return(list(
    coefficients = opt$par,
    fitted.values = as.vector(X %*% opt$par),
    convergence = opt$convergence,
    loss = opt$value
  ))
}

```

```{r Expectile regressions}
expectile_results <- list()

for (cur in names(alphas_clean)) {
  tau <- alphas_clean[[cur]]
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")

  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]])) %>%
    rename(y = all_of(excess_var), x = all_of(prem_var))

  X <- as.matrix(df$x)
  y <- df$y

  fit <- expectile_regression(X, y, tau = tau)
  forecast <- fit$fitted.values
  e <- y - forecast

  expectile_results[[cur]] <- tibble(
    forecast_error = e,
    premium = df$x,
    lag_error = lag(e),
    lag2_error = lag(e, 2),
    const = 1
  ) %>% drop_na()
}




```

```{r j-test for rational expectile forecast}
run_j_test <- function(data, alpha, p = 2) {
  # Moment function with no parameter estimation
  gmm_moments <- function(x) {
    e <- x[, "forecast_error"]
    z <- as.matrix(x[, c("const", "premium", "lag_error", "lag2_error")])
    lambda <- alpha + (1 - 2 * alpha) * (e < 0)
    g <- lambda * e^(p - 1) * z
    return(g)
  }

  # Now run GMM on data directly, using identity weighting
  moment_obj <- gmm_moment(g = gmm_moments, x = data)
  gmm_result <- specTest(moment_obj)
  return(gmm_result)
}

j_test_results <- list()

for (cur in names(expectile_results)) {
  message("Testing J-stat for ", cur)
  data <- expectile_results[[cur]]
  alpha <- alphas_clean[[cur]]

  res <- tryCatch({
    run_j_test(data, alpha)
  }, error = function(e) NULL)

  if (!is.null(res)) {
  j_test_results[[cur]] <- list(
    J_stat = res$statistic,
    J_pval = res$p.value
  )
}
}
j_df <- bind_rows(lapply(names(j_test_results), function(cur) {
  tibble(
    Currency = cur,
    J_stat = j_test_results[[cur]]$J_stat,
    p_value = j_test_results[[cur]]$J_pval
  )
}))


print(j_df)

```

```{r trying to install expectreg}
# install.packages(c("mboost", "BayesX", "fields"))
# install.packages("expectreg_0.53.tar.gz", repos = NULL, type = "source")
```

```{r Expectile based on expectreg}
expectile_regression_laws <- function(X, y, tau = 0.5, max_iter = 50, tol = 1e-6) {
  X <- as.matrix(X)
  n <- nrow(X)
  
  # Add intercept
  X <- cbind(1, X)
  
  # Initialize coefficients
  beta <- rep(0, ncol(X))
  
  for (i in 1:max_iter) {
    y_hat <- X %*% beta
    residuals <- y - y_hat
    
    weights <- ifelse(residuals >= 0, tau, 1 - tau)
    W <- diag(as.vector(weights))
    
    # Weighted least squares update
    beta_new <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y
    
    # Check convergence
    if (max(abs(beta_new - beta)) < tol) break
    beta <- beta_new
  }
  
  list(coefficients = beta, fitted = X %*% beta, residuals = y - X %*% beta)
}

```

```{r expectile test for GBP}
df_test <- wide_data %>%
  dplyr::select(GBP_excess, GBP_prem) %>%
  filter(!is.na(GBP_excess), !is.na(GBP_prem))

result <- expectile_regression_laws(X = df_test$GBP_prem, y = df_test$GBP_excess, tau = 0.3087742)

# Coefficients
print(result$coefficients)

# Plot fitted vs actual
plot(df_test$GBP_excess, result$fitted, 
     xlab = "Actual", ylab = "Fitted", main = "Expectile Regression (tau = 0.25)") +
abline(a = 0, b = 1, col = "red", lty = 2)

```

```{r Expectile Regression with fixed alphas}
fixed_expectiles <- c(0.1, 0.25, 0.5, 0.75, 0.9)
expectiles_by_currency <- setNames(as.numeric(alphas2), gsub("\\.Theta\\[1\\]", "", names(alphas2)))
currencies <- names(expectiles_by_currency)

# Store expectile regression results
er_results <- list()

for (cur in currencies) {
    cat("--------------", cur , "--------------\n")
  tau_vec <- sort(unique(c(expectiles_by_currency[[cur]], fixed_expectiles)))
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]])) %>%
    rename(y = all_of(excess_var), x = all_of(prem_var))
  
  X <- as.matrix(df$x)
  y <- df$y

  for (tau in tau_vec) {
      cat("tau: ", tau, "\n")
    fit <- expectile_regression(X, y, tau = tau)
    print(summary(fit))
    preds <- fit$fitted.values
    err <- y - preds

    er_results[[length(er_results) + 1]] <- list(
  Currency = cur,
  Tau = tau,
  Coef = fit$coefficients,
  Fitted = preds,
  Residuals = err,
  Metrics = tibble(
    Estimate = fit$coefficients[2],
    MAE = mean(abs(err)),
    RMSE = sqrt(mean(err^2)),
    AssymetricLoss = mean((tau + (1 - 2 * tau) * (err < 0)) * err^2)
  )
)

  }
}

# Combine results into a single dataframe
er_df <- bind_rows(lapply(er_results, function(x) {
  tibble(
    Currency = x$Currency,
    Tau = x$Tau,
    Estimate = x$Metrics$Estimate,
    MAE = x$Metrics$MAE,
    RMSE = x$Metrics$RMSE
  )
})) %>%
  arrange(Currency, Tau) %>%
  drop_na() %>%
  distinct()


# Highlight the optimal tau
highlight_er <- er_df %>%
  group_by(Currency) %>%
  filter(abs(Tau - expectiles_by_currency[Currency]) < 1e-6) %>%
  ungroup()

# Plot forecast error
ggplot(er_df, aes(x = Tau, y = MAE, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_point(data = highlight_er, shape = 21, size = 4, stroke = 1.5, fill = NA) +
  labs(title = "Expectile Regression Forecast Error by τ",
       x = "Expectile (τ)",
       y = "Mean Absolute Error (MAE)") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plot expectile slope estimates
ggplot(er_df, aes(x = Tau, y = Estimate, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Expectile Regression Coefficients by τ",
       x = "Expectile (τ)",
       y = expression(beta~"Estimate")) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

```{r plot expectiles - prem vs excess}
plot_expectiles <- function(df, expectile_fits, taus, currency = "GBP") {
  plot(df$x, df$y,
       main = paste("Expectile Regressions for", currency),
       xlab = "Forward Premium",
       ylab = "Excess Return",
       col = "grey70", pch = 16)

  cols <- hcl.colors(length(taus), "Dark 3")

  for (i in seq_along(taus)) {
    beta <- expectile_fits[[i]]$coefficients
    abline(a = beta[1], b = beta[2], col = cols[i], lwd = 2)
  }

  legend("topleft", legend = paste0("τ = ", taus),
         col = cols, lwd = 2, bty = "n")
}

# For a single currency
cur <- "GBP"
taus <- c(0.1, 0.25, 0.5, 0.75, 0.9)
fits <- list()
df <- wide_data %>%
  dplyr::select(GBP_excess, GBP_prem) %>%
  filter(!is.na(GBP_excess), !is.na(GBP_prem)) %>%
  rename(y = GBP_excess, x = GBP_prem)

for (tau in taus) {
  fits[[length(fits) + 1]] <- expectile_regression(X = as.matrix(df$x), y = df$y, tau = tau)
}

plot_expectiles(df, fits, taus, currency = "GBP")


```

```{r quantile regressions}

fixed_quantiles <- c(0.1, 0.25, 0.5, 0.75, 0.9)
quantiles_by_currency <- setNames(as.numeric(alphas2), gsub("\\.Theta\\[1\\]", "", names(alphas2)))
currencies <- names(quantiles_by_currency)

# Store results
qr_results <- list()

# Loop over currencies
for (cur in currencies) {
  tau_vec <- sort(unique(c(quantiles_by_currency[[cur]], fixed_quantiles)))
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]])) %>%
    rename(excess = all_of(excess_var), prem = all_of(prem_var))
  
  for (tau in tau_vec) {
    fit <- rq(excess ~ prem, tau = tau, data = df)
    preds <- predict(fit, newdata = df)
    err <- df$excess - preds
    
    qr_results[[length(qr_results) + 1]] <- tibble(
      Currency = cur,
      Tau = tau,
      Estimate = coef(fit)["prem"],
      SD = summary(fit)$coefficients["prem", "Std. Error"],
      MAE = mean(abs(err)),
      RMSE = sqrt(mean(err^2))
    )
  }
}

# Combine and inspect
qr_df <- bind_rows(qr_results)
qr_df <- qr_df %>% arrange(Currency, Tau) %>% drop_na() %>% distinct()
print(qr_df)

highlight_df <- qr_df %>%
  group_by(Currency) %>%
  filter(abs(Tau - quantiles_by_currency[Currency]) < 1e-6) %>%
  ungroup()

ggplot(qr_df, aes(x = Tau, y = MAE, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
    geom_point(data = highlight_df,
             aes(x = Tau, y = MAE, color = Currency),
             shape = 21, size = 4, stroke = 1.5, fill = NA) +
  labs(title = "Quantile Regression Forecast Error by τ",
       x = "Quantile (τ)",
       y = "Mean Absolute Error (MAE)") +
  theme_minimal() +
  theme(legend.position = "bottom")

ggplot(qr_df, aes(x = Tau, y = Estimate, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Quantile Regression Coefficients by τ",
       x = "Quantile (τ)",
       y = expression(beta~"Estimate")) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

```{r quantile regression 2}
qr_res_opt <- data.frame()

for (base in currencies) {
  cat("--------------", base , "--------------\n")
  
  d <- df1 %>% filter(Base == base)
  
  # Extract corresponding alpha (round name matching for robustness)
  alpha_tau <- alphas[grep(base, names(alphas), ignore.case = TRUE)]
  
  if (length(alpha_tau) == 0) {
    warning("No alpha found for base: ", base)
    next
  }
  
  tau <- as.numeric(alpha_tau)
  cat("Quantile (alpha-derived): ", tau, "\n")
  
  # Run quantile regression at behaviourally motivated tau
  q <- rq(excess ~ prem, data = d, tau = tau)
  s <- summary(q, se = "boot", R = 500)
  
  coef_est <- s$coefficients["prem", "Value"]
  coef_se <- s$coefficients["prem", "Std. Error"]
  
  # One-sided test: H0: beta >= 1, H1: beta < 1
  z <- (coef_est - 1) / coef_se
  p <- pnorm(z)
  
  qr_res_opt <- rbind(qr_res, data.frame(
    Base = base,
    Tau = tau,
    Estimate = coef_est,
    StdError = coef_se,
    Z = z,
    Pvalue = p
  ))
  
  cat("  Estimate:", round(coef_est, 4), 
      " | SE:", round(coef_se, 4),
      " | z:", round(z, 2),
      " | p-value (H0: beta >= 1):", round(p, 4), "\n\n")
}
```

```{r plotting quantile regression coefficients}
ggplot(qr_df, aes(x = Tau, y = Estimate)) +
  geom_point(color = "steelblue", size = 2) +
  geom_errorbar(aes(ymin = Estimate - SD, ymax = Estimate + SD),
                width = 0.02, color = "steelblue", linewidth = 0.6) +
  geom_line(color = "steelblue") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
    geom_point(data = highlight_df,
             aes(x = Tau, y = Estimate),
             shape = 21, size = 4, stroke = 1, fill = NA) +
  facet_wrap(~ Currency, scales = "free_y") +
  labs(title = "Quantile Regression Coefficients by τ",
       x = "Quantile (τ)",
       y = expression(beta~"Estimate")) +
  theme_minimal() +
  theme(strip.text = element_text(size = 12),
        legend.position = "none")



```

```{r quantile forests}
# install.packages("quantregForest")
# library(quantregForest)
df_gbp <- df1 %>% filter(Base == "GBP")

qrf_gbp <- quantregForest(x = data.frame(df_gbp$prem), 
                          y = df_gbp$excess,
                          nthreads = 4,
                          ntree = 1000, nodesize = 5)

print(qrf_gbp)

preds <- predict(qrf_gbp, newdata = data.frame(prem = df_gbp$prem), what = c(0.1, 0.5, 0.9))



```

```{r KT}
kt_value <- function(r, alpha = 0.88, lambda = 2.25) {
  ifelse(r >= 0, r^alpha, -lambda * (-r)^alpha)
}



df2 <- df1 %>%
    mutate(pt = kt_value(excess))

# plot excess returns vs pt
ggplot(df2, aes(x = excess, y = pt)) +
    geom_point(aes(color = Base), alpha = 0.5) +
    labs(title = "Excess Returns vs. Prospect Theory",
         x = "Excess Returns",
         y = "pt") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red", "green", "purple")) +
    theme(legend.position = "bottom")

```

```{r}

tune_pt_params_rolling <- function(data, alphas, lambdas, window_size = 60, horizon = 1) {
  param_grid <- expand.grid(alpha = alphas, lambda = lambdas)
  results <- data.frame()
  
  n <- nrow(data)
  max_start <- n - window_size - horizon + 1
  
  pb <- progress_bar$new(
    format = "  tuning [:bar] :percent eta: :eta",
    total = nrow(param_grid), clear = FALSE, width = 60
  )
  
  for (i in 1:nrow(param_grid)) {
    alpha <- param_grid$alpha[i]
    lambda <- param_grid$lambda[i]
    rmse_vec <- c()
    
    for (start in 1:max_start) {
      train_idx <- start:(start + window_size - 1)
      test_idx <- (start + window_size):(start + window_size + horizon - 1)
      
      train <- data[train_idx, ]
      test <- data[test_idx, ]
      
      model <- lm(kt_value(excess, alpha, lambda) ~ prem, data = train)
      preds <- predict(model, newdata = test)
      y_true <- kt_value(test$excess, alpha, lambda)
      rmse <- sqrt(mean((y_true - preds)^2))
      
      rmse_vec <- c(rmse_vec, rmse)
    }
    
    results <- rbind(results, data.frame(alpha, lambda, RMSE = mean(rmse_vec)))
    pb$tick()
  }
  
  best <- results[which.min(results$RMSE), ]
  return(list(best_params = best, full_results = results))
}


```

```{r Tuning value functions}
df_gbp <- df1 %>% filter(Base == "GBP")

alphas <- seq(0.5, 1.0, by = 0.05)
lambdas <- seq(0, 5.0, by = 0.25)

tuned <- tune_pt_params_rolling(df_gbp, alphas, lambdas)
best_params <- tuned$best_params

print(best_params)

```

```{r}
plot_value_function <- function(alphas = c(0.5, 0.88), lambdas = c(1, 2.25, 5), x_range = c(-2, 2)) {
  x_vals <- seq(x_range[1], x_range[2], length.out = 500)
  plot_data <- expand.grid(x = x_vals, alpha = alphas, lambda = lambdas)
  plot_data$value <- mapply(kt_value, plot_data$x, plot_data$alpha, plot_data$lambda)
  plot_data$label <- paste0("α=", plot_data$alpha, ", λ=", plot_data$lambda)
  
  ggplot(plot_data, aes(x = x, y = value, color = label)) +
    geom_line() +
    labs(
      title = "Prospect Theory Value Function",
      x = "Excess Return (x)",
      y = "Transformed Value v(x)",
      color = "Parameters"
    ) +
    theme_minimal()
}

# Example usage:
plot_value_function()

```


