---
title: "Dissertation Code v1"
author: "Archie C"
date: "2025-05-22"
output: html_document
---

```{r preamble, include=FALSE, message=FALSE, warning=FALSE}
source("preamble.R")

expectregurl <- "https://cran.r-project.org/src/contrib/Archive/expectreg/expectreg_0.53.tar.gz"
# install.packages(expectregurl, repos=NULL, type="source")
```


```{r Data Import, attr.message=FALSE, warning=FALSE, message=FALSE}
df <- read_xlsx("data/data1.xlsx")
dt <- as.data.table(df)
```

```{r wide format}
data <- df %>%
    mutate(Base = gsub("=", "", Instrument))

wide <- data %>%
    distinct() %>% # there are duplicates, but they are perfect duplicates so we take the distinct values
    dplyr::select(Base, Date, "Mid Price") %>%
    pivot_wider(names_from = Base, values_from = "Mid Price") %>%
    arrange(Date) %>%
    mutate(across(-Date, log))
# everything has been logged as we work in log(spot) and log(forward)
# now we need s_{t+1}

spot_lagged <- wide %>%
  transmute(Date = Date %m+% months(1),  # shift date back by 1 month to align with t
            GBP_t1 = GBP,
            EUR_t1 = EUR,
            JPY_t1 = JPY,
            CAD_t1 = CAD,
            GBP_FWD = GBP1MV,
            EUR_FWD = EUR1MV,
            JPY_FWD = JPY1MV,
            CAD_FWD = CAD1MV)

wide_data <- wide %>%
    inner_join(spot_lagged, by = "Date") %>%
    mutate(GBP_excess = GBP - GBP_t1,
           EUR_excess = EUR - EUR_t1,
           JPY_excess = JPY - JPY_t1,
           CAD_excess = CAD - CAD_t1,
           GBP_prem = GBP_FWD - GBP_t1,
           EUR_prem = EUR_FWD - EUR_t1,
           JPY_prem = JPY_FWD - JPY_t1,
           CAD_prem = CAD_FWD - CAD_t1)

```

```{r data vis}

long_spot <- wide %>%
  dplyr::select(Date, GBP, EUR, CAD) %>%  # drop JPY here
  pivot_longer(cols = -Date, names_to = "Base", values_to = "Spot")

long_prem <- wide_data %>%
  dplyr::select(Date, GBP_prem, EUR_prem, CAD_prem, JPY_prem) %>%  # drop JPY here
  pivot_longer(cols = -Date, names_to = "Base", values_to = "Prem")

ggplot(long_spot, aes(x = Date, y = Spot, color = Base)) +
    geom_line() +
    labs(title = "Spot Rates",
         x = "Date",
         y = "Premium") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red", "green", "purple")) +
    theme(legend.position = "bottom")

ggplot(long_prem, aes(x = Date, y = Prem)) +
    geom_line(aes(color = Base)) +
    labs(title = "Forward Premium",
         x = "Date",
         y = "Premium") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red", "green", "purple")) +
    theme(legend.position = "bottom")
```

```{r Linear Model}
lm_data <- list()
for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)

  summary_stats <- tibble(
    Currency   = cur,
    Intercept  = coef(model)[1],
    SE.Intercept = summary(model)$coefficients[1, 2],
    Slope      = coef(model)[2],
    SE.Slope   = summary(model)$coefficients[2, 2],
    R2         = summary(model)$r.squared,
    N          = nobs(model)
  )
  
  fitted_df <- tibble(
    Currency = cur,
    Fitted   = fitted(model),
    Residual = resid(model)
  )
  
  lm_data[[cur]] <- list(summary = summary_stats, residuals = fitted_df)
}


lmresults_df <- bind_rows(list(lm_data$GBP$summary, lm_data$EUR$summary, lm_data$CAD$summary, lm_data$JPY$summary))
print(lmresults_df)

```


```{r Checking for autocorrelation in the errors}
for (cur in currencies) {
    cat("------", cur, "------\n")
  model <- lm(as.formula(paste0(cur, "_excess ~ ", cur, "_prem")), data = wide_data)
  print(cur)
  print(Box.test(resid(model), lag = 22, type = "Ljung-Box"))
}

par(mfrow = c(2, 2))  # 2x2 grid for 4 plots
for (cur in currencies) {
  model <- lm(as.formula(paste0(cur, "_excess ~ ", cur, "_prem")), data = wide_data)
  res <- resid(model)
  acf(res, lag.max = 40, main = paste("ACF of Residuals:", cur))
}

```

```{r}
library(forecast)

for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")

  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    dplyr::filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))

  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)
  res <- resid(model)

  cat("-----", cur, "-----\n")
  auto_model <- auto.arima(res, max.p = 0, max.q = 30, seasonal = FALSE, ic = "bic")
  print(auto_model)
}

```

```{r Newey-West Standard Errors - everything insig}

lmnw_data <- list()
for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = df)
  nw_vcov <- NeweyWest(model, lag = 14, prewhite = FALSE)
  nw_se <- coeftest(model, vcov. = nw_vcov)

  summary_stats <- tibble(
    Currency   = cur,
    Intercept  = coef(model)[1],
    SE.Intercept = nw_se[1, "Std. Error"],
    Slope      = coef(model)[2],
    SE.Slope   = nw_se[2, "Std. Error"],
    R2         = summary(model)$r.squared,
    N          = nobs(model)
  )
  
  fitted_df <- tibble(
    Currency = cur,
    Fitted   = fitted(model),
    Residual = resid(model)
  )
  
  lmnw_data[[cur]] <- list(summary = summary_stats, residuals = fitted_df)
}
lmnwresults_df <- bind_rows(list(lmnw_data$GBP$summary, lmnw_data$EUR$summary, lmnw_data$CAD$summary, lmnw_data$JPY$summary))
print(lmnwresults_df)

```


```{r Plotting lm Residuals}

# Combine all residuals
resid_plot_df <- bind_rows(list(lm_data$GBP$residuals,
                           lm_data$EUR$residuals,
                           lm_data$CAD$residuals,
                           lm_data$JPY$residuals))

means_sds <- resid_plot_df %>%
  group_by(Currency) %>%
  summarise(Mean = unique(Mean), SD = unique(SD), .groups = "drop")

normal_curves <- means_sds %>%
  mutate(x = map2(Mean, SD, ~ seq(.x - 4*.y, .x + 4*.y, length.out = 200))) %>%
  unnest(cols = c(x)) %>%
  mutate(y = dnorm(x, mean = Mean, sd = SD))

# Now plot
ggplot(resid_plot_df, aes(x = Residual)) +
  geom_density(color = "darkred", linewidth = 1, fill = "darkred", alpha = 0.5) +
  geom_line(data = normal_curves, aes(x = x, y = y), color = "blue", linetype = "dashed", linewidth = 0.5, alpha = 0.9) +
  facet_wrap(~ Currency, scales = "free") +
  theme_minimal() +
  labs(title = "Prediction Error Densities vs Fitted Normal Distribution",
       x = "Residual",
       y = "Density")

```


```{r simple regression to check on the forward premium bias}
currencies <- c("GBP", "EUR", "CAD", "JPY")

# Construct tidy summary for each currency
simple <- map_dfr(currencies, function(cur) {
  excess_var <- paste0(cur, "_excess")
  prem_var <- paste0(cur, "_prem")
  
  data_cur <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  model <- lm(as.formula(paste0(excess_var, " ~ ", prem_var)), data = data_cur)
  tidy(model) %>% mutate(Currency = cur)
})

for (base in currencies) {
  cat("--------------", base , "--------------\n")
  
  excess_var <- paste0(base, "_excess")
  prem_var <- paste0(base, "_prem")
  
  # Keep only relevant rows with no NA
  df <- wide_data %>%
    dplyr::select(Date, all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]]))
  
  # Dynamically construct the formula
  f <- as.formula(paste0(excess_var, " ~ ", prem_var))
  m <- lm(f, data = df)
  
  cat("  alpha:", round(coef(m)[1], 4), 
      " | beta:", round(coef(m)[2], 4), "\n")
  
  # Run hypothesis test: H0: beta = 1
  print(linearHypothesis(m, paste0(prem_var, " = 1")))
}
```


So we have found recreated some of the results. We see that in the case of Canada, the result is not statistically different from 1, but in the case of the other currencies, we see that the forward premium is statistically significantly different from 1. This is consistent with the findings in the literature.

```{r plot of coefficients from simple regression}
ggplot(lmresults_df, aes(x = Currency, y = Slope)) +
  geom_point() +
  geom_errorbar(aes(ymin = Slope - 1.96 * SE.Slope,
                    ymax = Slope + 1.96 * SE.Slope),
                width = 0.1) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "grey") +
  labs(title = "Simple Regression Estimates",
       x = "Currency", y = expression(hat(beta) ~ "(on forward premium)")) +
  theme_minimal()
```

### Estimating the asymmetric loss function

```{r estimating the loss functions}

estimate_a <- function(currencies, wide_data, p) {
  # Define moment function inside for clarity/scope
  gmm_moments <- function(theta, data) {
    alpha <- theta[1]
    e <- data$forecast_error
    z <- as.matrix(data[, c("const", "lag_error")])
    lambda <- alpha + (1 - 2 * alpha) * (e < 0)
    g <- lambda * e^(p - 1) * z
    return(g)
  }

  currency_results <- list()

  for (cur in currencies) {
    message("Running GMM for ", cur)

    e_col <- cur
    f_col <- paste0(cur, "_FWD")

    data_cur <- wide_data %>%
      transmute(
        forecast_error = .data[[e_col]] - .data[[f_col]],
        lag_error = lag(.data[[e_col]] - .data[[f_col]]),
        const = 1
      ) %>%
      na.omit()

    if (nrow(data_cur) > 10) {
      res <- tryCatch({
        gmm(g = gmm_moments,
            x = data_cur,
            t0 = c(0.5),
            type = "cue",
            method = "Brent",
            lower = 0.01,
            upper = 0.99)
      }, error = function(e) NULL)

      if (!is.null(res)) {
        alpha_hat <- coef(res)
        vcv_hat <- vcov(res)
        se_alpha <- sqrt(vcv_hat[1, 1])
        t_stat <- (alpha_hat - 0.5) / se_alpha
        p_val <- 2 * pt(-abs(t_stat), df = nrow(data_cur) - 1)

        currency_results[[cur]] <- list(
          summary = summary(res),
          alpha = alpha_hat,
          vcov = vcv_hat,
          t_stat = t_stat,
          p_val = p_val
        )
      }
    } else {
      message("Not enough data for ", cur)
    }
  }

  return(currency_results)
}

results2 <- estimate_a(currencies = currencies, wide_data = wide_data, p = 2)


# View results for one example currency (e.g., EUR)
results2$EUR
results2$GBP
results2$CAD
results2$JPY


# Extract alpha estimates
alphas2 <- sapply(results2, function(x) if (!is.null(x)) x$alpha else NA)
print(alphas2)

# Extract p-values for H0: alpha = 0.5
p_vals2 <- sapply(results2, function(x) if (!is.null(x)) x$p_val else NA)
print(p_vals2)


```

```{r transformed excess returns lm}
# Step 1: Clean alpha names
alphas_clean <- setNames(as.numeric(alphas2), gsub("\\.Theta\\[1\\]", "", names(alphas2)))

# Step 2: Transform excess returns using loss function
wide_transformed <- wide_data

for (cur in currencies) {
  excess_var <- paste0(cur, "_excess")
  trans_var  <- paste0(cur, "_excess_trans")

  if (cur %in% names(alphas_clean)) {
    alpha <- alphas_clean[[cur]]

    e <- wide_transformed[[excess_var]]
    wide_transformed[[trans_var]] <- (alpha + (1 - 2 * alpha) * (e < 0)) * abs(e)^2
  } else {
    wide_transformed[[trans_var]] <- NA_real_
  }
}

simple2 <- purrr::map_dfr(currencies, function(cur) {
  trans_var <- paste0(cur, "_excess_trans")
  prem_var  <- paste0(cur, "_prem")

  df <- wide_transformed %>%
    dplyr::select(all_of(c(trans_var, prem_var))) %>%
    filter(!is.na(.data[[trans_var]]), !is.na(.data[[prem_var]]))

  model <- lm(as.formula(paste0(trans_var, " ~ ", prem_var)), data = df)
  tidy(model) %>% mutate(Currency = cur)
})

# Step 4: Print results with hypothesis tests
for (cur in currencies) {
  cat("--------------", cur , "--------------\n")

  trans_var <- paste0(cur, "_excess_trans")
  prem_var  <- paste0(cur, "_prem")

  df <- wide_transformed %>%
    dplyr::select(all_of(c(trans_var, prem_var))) %>%
    filter(!is.na(.data[[trans_var]]), !is.na(.data[[prem_var]]))

  m <- lm(as.formula(paste0(trans_var, " ~ ", prem_var)), data = df)

  cat("  alpha:", round(coef(m)[1], 4), 
      " | beta:", round(coef(m)[2], 4), "\n")

  print(summary(m))
}

```





```{r Expectile Regression Function}
expectile_regression <- function(X, y, tau = 0.5) {
  X <- as.matrix(X)
  y <- as.numeric(y)
  X <- cbind(1, X)  
  n <- nrow(X)
  p <- ncol(X)
  
  # Loss function - quad-quad loss function.
  
  loss_fn <- function(beta) {
    y_hat <- X %*% beta
    e <- y - y_hat
    weights <- ifelse(e < 0, 1 - tau, tau)
    mean(weights * e^2)
  }
  
  # Gradient function
  grad_fn <- function(beta) {
    y_hat <- X %*% beta
    e <- y - y_hat
    weights <- ifelse(e < 0, 1 - tau, tau)
    grad <- -2 * t(X) %*% (weights * e) / n
    return(as.vector(grad))
  }
  
  # Optimize
  opt <- optim(
    par = rep(0, p),
    fn = loss_fn,
    gr = grad_fn,
    method = "BFGS",
    control = list(maxit = 500, reltol = 1e-8)
  )
  
  return(list(
    coefficients = opt$par,
    fitted.values = as.vector(X %*% opt$par),
    convergence = opt$convergence,
    loss = opt$value
  ))
}

```

```{r Expectile regressions}
expectile_results <- list()

for (cur in names(alphas_clean)) {
  tau <- alphas_clean[[cur]]
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")

  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]])) %>%
    rename(y = all_of(excess_var), x = all_of(prem_var))

  X <- as.matrix(df$x)
  y <- df$y

  fit <- expectile_regression(X, y, tau = tau)
  forecast <- fit$fitted.values
  e <- y - forecast

  expectile_results[[cur]] <- tibble(
    forecast_error = e,
    premium = df$x,
    lag_error = lag(e),
    lag2_error = lag(e, 2),
    const = 1
  ) %>% drop_na()
}




```

```{r j-test for rational expectile forecast}
run_j_test <- function(data, alpha, p = 2) {
  # Moment function with no parameter estimation
  gmm_moments <- function(x) {
    e <- x[, "forecast_error"]
    z <- as.matrix(x[, c("const", "premium", "lag_error", "lag2_error")])
    lambda <- alpha + (1 - 2 * alpha) * (e < 0)
    g <- lambda * e^(p - 1) * z
    return(g)
  }

  # Now run GMM on data directly, using identity weighting
  moment_obj <- gmm_moment(g = gmm_moments, x = data)
  gmm_result <- specTest(moment_obj)
  return(gmm_result)
}

j_test_results <- list()

for (cur in names(expectile_results)) {
  message("Testing J-stat for ", cur)
  data <- expectile_results[[cur]]
  alpha <- alphas_clean[[cur]]

  res <- tryCatch({
    run_j_test(data, alpha)
  }, error = function(e) NULL)

  if (!is.null(res)) {
  j_test_results[[cur]] <- list(
    J_stat = res$statistic,
    J_pval = res$p.value
  )
}
}
j_df <- bind_rows(lapply(names(j_test_results), function(cur) {
  tibble(
    Currency = cur,
    J_stat = j_test_results[[cur]]$J_stat,
    p_value = j_test_results[[cur]]$J_pval
  )
}))


print(j_df)

```



```{r}
# install.packages(c("mboost", "BayesX", "fields"))
# install.packages("expectreg_0.53.tar.gz", repos = NULL, type = "source")
```

```{r Expectile based on package code}
expectile_regression_laws <- function(X, y, tau = 0.5, max_iter = 50, tol = 1e-6) {
  X <- as.matrix(X)
  n <- nrow(X)
  
  # Add intercept
  X <- cbind(1, X)
  
  # Initialize coefficients
  beta <- rep(0, ncol(X))
  
  for (i in 1:max_iter) {
    y_hat <- X %*% beta
    residuals <- y - y_hat
    
    weights <- ifelse(residuals >= 0, tau, 1 - tau)
    W <- diag(as.vector(weights))
    
    # Weighted least squares update
    beta_new <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y
    
    # Check convergence
    if (max(abs(beta_new - beta)) < tol) break
    beta <- beta_new
  }
  
  list(coefficients = beta, fitted = X %*% beta, residuals = y - X %*% beta)
}

```

```{r}
# Suppose you want expectile regression for GBP
df_test <- wide_data %>%
  dplyr::select(GBP_excess, GBP_prem) %>%
  filter(!is.na(GBP_excess), !is.na(GBP_prem))

result <- expectile_regression_laws(X = df_test$GBP_prem, y = df_test$GBP_excess, tau = 0.3087742)

# Coefficients
print(result$coefficients)

# Plot fitted vs actual
plot(df_test$GBP_excess, result$fitted, 
     xlab = "Actual", ylab = "Fitted", main = "Expectile Regression (tau = 0.25)") +
abline(a = 0, b = 1, col = "red", lty = 2)

```

```{r Expectile Regression with fixed alphas}
fixed_expectiles <- c(0.1, 0.25, 0.5, 0.75, 0.9)
expectiles_by_currency <- setNames(as.numeric(alphas2), gsub("\\.Theta\\[1\\]", "", names(alphas2)))
currencies <- names(expectiles_by_currency)

# Store expectile regression results
er_results <- list()

for (cur in currencies) {
    cat("--------------", cur , "--------------\n")
  tau_vec <- sort(unique(c(expectiles_by_currency[[cur]], fixed_expectiles)))
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]])) %>%
    rename(y = all_of(excess_var), x = all_of(prem_var))
  
  X <- as.matrix(df$x)
  y <- df$y

  for (tau in tau_vec) {
      cat("tau: ", tau, "\n")
    fit <- expectile_regression(X, y, tau = tau)
    print(summary(fit))
    preds <- fit$fitted.values
    err <- y - preds

    er_results[[length(er_results) + 1]] <- list(
  Currency = cur,
  Tau = tau,
  Coef = fit$coefficients,
  Fitted = preds,
  Residuals = err,
  Metrics = tibble(
    Estimate = fit$coefficients[2],
    MAE = mean(abs(err)),
    RMSE = sqrt(mean(err^2)),
    AssymetricLoss = mean((tau + (1 - 2 * tau) * (err < 0)) * err^2)
  )
)

  }
}

# Combine results into a single dataframe
er_df <- bind_rows(lapply(er_results, function(x) {
  tibble(
    Currency = x$Currency,
    Tau = x$Tau,
    Estimate = x$Metrics$Estimate,
    MAE = x$Metrics$MAE,
    RMSE = x$Metrics$RMSE
  )
})) %>%
  arrange(Currency, Tau) %>%
  drop_na() %>%
  distinct()


# Highlight the optimal tau
highlight_er <- er_df %>%
  group_by(Currency) %>%
  filter(abs(Tau - expectiles_by_currency[Currency]) < 1e-6) %>%
  ungroup()

# Plot forecast error
ggplot(er_df, aes(x = Tau, y = MAE, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_point(data = highlight_er, shape = 21, size = 4, stroke = 1.5, fill = NA) +
  labs(title = "Expectile Regression Forecast Error by τ",
       x = "Expectile (τ)",
       y = "Mean Absolute Error (MAE)") +
  theme_minimal() +
  theme(legend.position = "bottom")

# Plot expectile slope estimates
ggplot(er_df, aes(x = Tau, y = Estimate, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Expectile Regression Coefficients by τ",
       x = "Expectile (τ)",
       y = expression(beta~"Estimate")) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

```{r}
plot_expectiles <- function(df, expectile_fits, taus, currency = "GBP") {
  plot(df$x, df$y,
       main = paste("Expectile Regressions for", currency),
       xlab = "Forward Premium",
       ylab = "Excess Return",
       col = "grey70", pch = 16)

  cols <- hcl.colors(length(taus), "Dark 3")

  for (i in seq_along(taus)) {
    beta <- expectile_fits[[i]]$coefficients
    abline(a = beta[1], b = beta[2], col = cols[i], lwd = 2)
  }

  legend("topleft", legend = paste0("τ = ", taus),
         col = cols, lwd = 2, bty = "n")
}

# For a single currency
cur <- "GBP"
taus <- c(0.1, 0.25, 0.5, 0.75, 0.9)
fits <- list()
df <- wide_data %>%
  dplyr::select(GBP_excess, GBP_prem) %>%
  filter(!is.na(GBP_excess), !is.na(GBP_prem)) %>%
  rename(y = GBP_excess, x = GBP_prem)

for (tau in taus) {
  fits[[length(fits) + 1]] <- expectile_regression(X = as.matrix(df$x), y = df$y, tau = tau)
}

plot_expectiles(df, fits, taus, currency = "GBP")


```




```{r quantile regressions}

fixed_quantiles <- c(0.1, 0.25, 0.5, 0.75, 0.9)
quantiles_by_currency <- setNames(as.numeric(alphas2), gsub("\\.Theta\\[1\\]", "", names(alphas2)))
currencies <- names(quantiles_by_currency)

# Store results
qr_results <- list()

# Loop over currencies
for (cur in currencies) {
  tau_vec <- sort(unique(c(quantiles_by_currency[[cur]], fixed_quantiles)))
  excess_var <- paste0(cur, "_excess")
  prem_var   <- paste0(cur, "_prem")
  
  df <- wide_data %>%
    dplyr::select(all_of(c(excess_var, prem_var))) %>%
    filter(!is.na(.data[[excess_var]]), !is.na(.data[[prem_var]])) %>%
    rename(excess = all_of(excess_var), prem = all_of(prem_var))
  
  for (tau in tau_vec) {
    fit <- rq(excess ~ prem, tau = tau, data = df)
    preds <- predict(fit, newdata = df)
    err <- df$excess - preds
    
    qr_results[[length(qr_results) + 1]] <- tibble(
      Currency = cur,
      Tau = tau,
      Estimate = coef(fit)["prem"],
      SD = summary(fit)$coefficients["prem", "Std. Error"],
      MAE = mean(abs(err)),
      RMSE = sqrt(mean(err^2))
    )
  }
}

# Combine and inspect
qr_df <- bind_rows(qr_results)
qr_df <- qr_df %>% arrange(Currency, Tau) %>% drop_na() %>% distinct()
print(qr_df)

highlight_df <- qr_df %>%
  group_by(Currency) %>%
  filter(abs(Tau - quantiles_by_currency[Currency]) < 1e-6) %>%
  ungroup()

ggplot(qr_df, aes(x = Tau, y = MAE, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
    geom_point(data = highlight_df,
             aes(x = Tau, y = MAE, color = Currency),
             shape = 21, size = 4, stroke = 1.5, fill = NA) +
  labs(title = "Quantile Regression Forecast Error by τ",
       x = "Quantile (τ)",
       y = "Mean Absolute Error (MAE)") +
  theme_minimal() +
  theme(legend.position = "bottom")

ggplot(qr_df, aes(x = Tau, y = Estimate, color = Currency)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  labs(title = "Quantile Regression Coefficients by τ",
       x = "Quantile (τ)",
       y = expression(beta~"Estimate")) +
  theme_minimal() +
  theme(legend.position = "bottom")

```




```{r quantile regression 2}
qr_res_opt <- data.frame()

for (base in currencies) {
  cat("--------------", base , "--------------\n")
  
  d <- df1 %>% filter(Base == base)
  
  # Extract corresponding alpha (round name matching for robustness)
  alpha_tau <- alphas[grep(base, names(alphas), ignore.case = TRUE)]
  
  if (length(alpha_tau) == 0) {
    warning("No alpha found for base: ", base)
    next
  }
  
  tau <- as.numeric(alpha_tau)
  cat("Quantile (alpha-derived): ", tau, "\n")
  
  # Run quantile regression at behaviourally motivated tau
  q <- rq(excess ~ prem, data = d, tau = tau)
  s <- summary(q, se = "boot", R = 500)
  
  coef_est <- s$coefficients["prem", "Value"]
  coef_se <- s$coefficients["prem", "Std. Error"]
  
  # One-sided test: H0: beta >= 1, H1: beta < 1
  z <- (coef_est - 1) / coef_se
  p <- pnorm(z)
  
  qr_res_opt <- rbind(qr_res, data.frame(
    Base = base,
    Tau = tau,
    Estimate = coef_est,
    StdError = coef_se,
    Z = z,
    Pvalue = p
  ))
  
  cat("  Estimate:", round(coef_est, 4), 
      " | SE:", round(coef_se, 4),
      " | z:", round(z, 2),
      " | p-value (H0: beta >= 1):", round(p, 4), "\n\n")
}
```

```{r plotting quantile regression coefficients}
ggplot(qr_df, aes(x = Tau, y = Estimate)) +
  geom_point(color = "steelblue", size = 2) +
  geom_errorbar(aes(ymin = Estimate - SD, ymax = Estimate + SD),
                width = 0.02, color = "steelblue", linewidth = 0.6) +
  geom_line(color = "steelblue") +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
    geom_point(data = highlight_df,
             aes(x = Tau, y = Estimate),
             shape = 21, size = 4, stroke = 1, fill = NA) +
  facet_wrap(~ Currency, scales = "free_y") +
  labs(title = "Quantile Regression Coefficients by τ",
       x = "Quantile (τ)",
       y = expression(beta~"Estimate")) +
  theme_minimal() +
  theme(strip.text = element_text(size = 12),
        legend.position = "none")



```



```{r quantile forests}
# install.packages("quantregForest")
# library(quantregForest)
df_gbp <- df1 %>% filter(Base == "GBP")

qrf_gbp <- quantregForest(x = data.frame(df_gbp$prem), 
                          y = df_gbp$excess,
                          nthreads = 4,
                          ntree = 1000, nodesize = 5)

print(qrf_gbp)

preds <- predict(qrf_gbp, newdata = data.frame(prem = df_gbp$prem), what = c(0.1, 0.5, 0.9))



```


```{r}
kt_value <- function(r, alpha = 0.88, lambda = 2.25) {
  ifelse(r >= 0, r^alpha, -lambda * (-r)^alpha)
}



df2 <- df1 %>%
    mutate(pt = kt_value(excess))

# plot excess returns vs pt
ggplot(df2, aes(x = excess, y = pt)) +
    geom_point(aes(color = Base), alpha = 0.5) +
    labs(title = "Excess Returns vs. Prospect Theory",
         x = "Excess Returns",
         y = "pt") +
    theme_minimal() +
    scale_color_manual(values = c("blue", "red", "green", "purple")) +
    theme(legend.position = "bottom")

```

```{r}

tune_pt_params_rolling <- function(data, alphas, lambdas, window_size = 60, horizon = 1) {
  param_grid <- expand.grid(alpha = alphas, lambda = lambdas)
  results <- data.frame()
  
  n <- nrow(data)
  max_start <- n - window_size - horizon + 1
  
  pb <- progress_bar$new(
    format = "  tuning [:bar] :percent eta: :eta",
    total = nrow(param_grid), clear = FALSE, width = 60
  )
  
  for (i in 1:nrow(param_grid)) {
    alpha <- param_grid$alpha[i]
    lambda <- param_grid$lambda[i]
    rmse_vec <- c()
    
    for (start in 1:max_start) {
      train_idx <- start:(start + window_size - 1)
      test_idx <- (start + window_size):(start + window_size + horizon - 1)
      
      train <- data[train_idx, ]
      test <- data[test_idx, ]
      
      model <- lm(kt_value(excess, alpha, lambda) ~ prem, data = train)
      preds <- predict(model, newdata = test)
      y_true <- kt_value(test$excess, alpha, lambda)
      rmse <- sqrt(mean((y_true - preds)^2))
      
      rmse_vec <- c(rmse_vec, rmse)
    }
    
    results <- rbind(results, data.frame(alpha, lambda, RMSE = mean(rmse_vec)))
    pb$tick()
  }
  
  best <- results[which.min(results$RMSE), ]
  return(list(best_params = best, full_results = results))
}


```

```{r Tuning value functions}
df_gbp <- df1 %>% filter(Base == "GBP")

alphas <- seq(0.5, 1.0, by = 0.05)
lambdas <- seq(0, 5.0, by = 0.25)

tuned <- tune_pt_params_rolling(df_gbp, alphas, lambdas)
best_params <- tuned$best_params

print(best_params)

```

```{r}


plot_value_function <- function(alphas = c(0.5, 0.88), lambdas = c(1, 2.25, 5), x_range = c(-2, 2)) {
  x_vals <- seq(x_range[1], x_range[2], length.out = 500)
  plot_data <- expand.grid(x = x_vals, alpha = alphas, lambda = lambdas)
  plot_data$value <- mapply(kt_value, plot_data$x, plot_data$alpha, plot_data$lambda)
  plot_data$label <- paste0("α=", plot_data$alpha, ", λ=", plot_data$lambda)
  
  ggplot(plot_data, aes(x = x, y = value, color = label)) +
    geom_line() +
    labs(
      title = "Prospect Theory Value Function",
      x = "Excess Return (x)",
      y = "Transformed Value v(x)",
      color = "Parameters"
    ) +
    theme_minimal()
}

# Example usage:
plot_value_function()

```


# Deep Parametric Porfolio Policy
Given we have these alphas that we are estimating we can see that based on the forward rates, some currencies are systematically under-/over- predicted. We aim to utilise this fact and optimise portfolio allocation based on these utiltiy asymmetries calculated on a rolling basis.

```{r Sim data}
currs <- c("GBP", "EUR", "JPY", "CAD")
n <- nrow(wide_data)

simulate_macro <- function(base, vol, n) {
  rnorm(n, mean = base, sd = vol)
}

wide_data2 <- wide_data %>%
  mutate(
    int_US = simulate_macro(0.015, 0.002, n),
    inf_US = simulate_macro(0.02,  0.002, n),
    gdp_US = simulate_macro(0.025, 0.003, n)
  )

# Simulate for each currency and compute differentials
for (ccy in currs) {
  wide_data2[[paste0("int_", ccy)]] <- simulate_macro(0.02, 0.0025, n)
  wide_data2[[paste0("inf_", ccy)]] <- simulate_macro(0.025, 0.0025, n)
  wide_data2[[paste0("gdp_", ccy)]] <- simulate_macro(0.03, 0.003, n)
  
  # Compute differentials
  wide_data2[[paste0("int_diff_", ccy)]] <- wide_data2[[paste0("int_", ccy)]] - wide_data2$int_US
  wide_data2[[paste0("inf_diff_", ccy)]] <- wide_data2[[paste0("inf_", ccy)]] - wide_data2$inf_US
  wide_data2[[paste0("gdp_diff_", ccy)]] <- wide_data2[[paste0("gdp_", ccy)]] - wide_data2$gdp_US
}

```


```{r D-PPP Trading Strategy}
library(keras)
library(tensorflow)


# -------- 0. User inputs --------
# wide_data: Must contain Date, for each currency: XX_excess, XX_FWD, and macro features
# currencies: e.g. c("GBP", "EUR", "JPY", "CAD")
# window: size of rolling GMM window (in months)
# p: power for loss (1 = lin-lin, 2 = quad-quad)

currencies <- c("GBP","EUR","JPY","CAD")  # <-- user-defined
window <- 60   # e.g. 5 years of monthly data
p <- 2         # quad-quad loss
macro_prefixes <- c() # (optional) if you want to filter macro features by prefix, e.g. c("infl_", "gdp_")

# -------- 1. Identify Macro Features Automatically --------
exclude_patterns <- c("_excess$", "_FWD$", "^Date$", "^date$", "^const$", "^lag_")
macro_features <- names(wide_data)
for (pat in exclude_patterns) {
  macro_features <- macro_features[!grepl(pat, macro_features)]
}
if (length(macro_prefixes) > 0) {
  macro_features <- macro_features[grepl(paste(macro_prefixes, collapse="|"), macro_features)]
}
# Remove any duplicated names just in case
macro_features <- unique(macro_features)

# -------- 2. Rolling GMM Alpha Estimation (Elliott et al. 2006) --------
roll_alpha_gmm <- function(data, currencies, window = 60, p = 2) {
  n <- nrow(data)
  date_seq <- data$Date[(window + 1):n]
  alpha_matrix <- matrix(NA, nrow = length(date_seq), ncol = length(currencies),
                         dimnames = list(as.character(date_seq), currencies))
  for (i in seq_along(date_seq)) {
    idx_start <- i
    idx_end <- i + window - 1
    sub_data <- data[idx_start:idx_end, ]
    for (cur in currencies) {
      e_col <- paste0(cur, "_excess")
      f_col <- paste0(cur, "_FWD")
      dat_cur <- sub_data %>%
        transmute(
          forecast_error = .data[[e_col]] - .data[[f_col]],
          lag_error = lag(.data[[e_col]] - .data[[f_col]]),
          const = 1
        ) %>% na.omit()
      if (nrow(dat_cur) > 10) {
        gmm_moments <- function(theta, data) {
          alpha <- theta[1]
          e <- data$forecast_error
          z <- as.matrix(data[, c("const", "lag_error")])
          lambda <- alpha + (1 - 2 * alpha) * (e < 0)
          g <- lambda * e^(p - 1) * z
          return(g)
        }
        res <- tryCatch({
          gmm(g = gmm_moments, x = dat_cur, t0 = c(0.5),
              type = "cue", method = "Brent", lower = 0.01, upper = 0.99)
        }, error = function(e) NULL)
        alpha_matrix[i, cur] <- if (!is.null(res)) coef(res)[1] else NA
      }
    }
  }
  alpha_matrix
}

alpha_matrix <- roll_alpha_gmm(wide_data, currencies, window = window, p = p)
date_seq <- as.Date(rownames(alpha_matrix))
n_steps <- length(date_seq)
N_assets <- length(currencies)

# -------- 3. Build Features and Target Arrays for Neural Net --------
F_feats <- length(macro_features) + 2  # fwd prem + alpha + all macro features

X_feats <- array(NA_real_, dim = c(n_steps, N_assets, F_feats),
                 dimnames = list(time = as.character(date_seq), asset = currencies,
                                 feature = c("fwd_prem", macro_features, "alpha")))
R_next  <- matrix(NA_real_, nrow = n_steps, ncol = N_assets, dimnames = list(as.character(date_seq), currencies))

for (i in seq_len(n_steps)) {
  t <- date_seq[i]
  t1 <- t %m+% months(1)
  row <- wide_data %>% filter(Date == t)
  next_row <- wide_data %>% filter(Date == t1)
  for (j in seq_along(currencies)) {
    cur <- currencies[j]
    fwd_prem <- if (!is.null(row[[paste0(cur, "_FWD")]]) & !is.null(row[[paste0(cur, "_excess")]])) {
      row[[paste0(cur, "_FWD")]] - row[[paste0(cur, "_excess")]]
    } else { NA_real_ }
    macro_vals <- as.numeric(row[1, macro_features, drop = FALSE])
    alpha_val <- alpha_matrix[i, cur]
    X_feats[i, j, ] <- c(fwd_prem, macro_vals, alpha_val)
    R_next[i, j] <- if (nrow(next_row) > 0) next_row[[paste0(cur, "_excess")]] else NA
  }
}

# Remove rows where all R_next are NA (no returns to predict)
valid_idx <- which(!apply(is.na(R_next), 1, all))
X_feats_final     <- X_feats[valid_idx, , , drop = FALSE]
R_next_final      <- R_next[valid_idx, , drop = FALSE]
train_dates_final <- date_seq[valid_idx]

# Optional: Remove rows where ANY macro feature is NA
good_rows <- sapply(seq_len(dim(X_feats_final)[1]), function(i) !any(is.na(X_feats_final[i, , ])))
X_feats_final     <- X_feats_final[good_rows, , , drop = FALSE]
R_next_final      <- R_next_final[good_rows, , drop = FALSE]
train_dates_final <- train_dates_final[good_rows]

# -------- 4. Keras Deep PPP Model Definition --------
F_feats <- dim(X_feats_final)[3]
N_assets <- dim(X_feats_final)[2]

asset_net <- keras_model_sequential(name = 'asset_net') %>%
  layer_dense(units = 32, kernel_regularizer = regularizer_l2(1e-4), input_shape = c(F_feats)) %>%
  layer_activation_leaky_relu(alpha = 0.01) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(1e-4)) %>%
  layer_activation_leaky_relu(alpha = 0.01) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1, activation = 'linear', kernel_regularizer = regularizer_l2(1e-4))

input_layer <- layer_input(shape = c(N_assets, F_feats), name = 'features')
scores      <- input_layer %>% time_distributed(asset_net)
scores_flat <- scores %>% layer_flatten()
g_avg       <- scores_flat %>% layer_lambda(function(x) k_mean(x, axis = 1L, keepdims = TRUE))
g_centered  <- list(scores_flat, g_avg) %>%
  layer_lambda(function(lst) lst[[1]] - lst[[2]])
one_over_N  <- 1.0 / as.numeric(N_assets)
weights_out <- g_centered %>%
  layer_lambda(function(x) x * one_over_N + one_over_N, name = 'weights')
model <- keras_model(inputs = input_layer, outputs = weights_out)

rr <- 5
cap_percent <- 0.03
lambda_max  <- 50
dcrra_loss <- function(y_true, y_pred) {
  r_p       <- k_sum(y_pred * y_true, axis = 1L)
  U         <- k_pow(1 + r_p, 1 - rr) / (1 - rr)
  base_loss <- -k_mean(U)
  excess_pos <- k_relu(y_pred - cap_percent)
  excess_neg <- k_relu(-cap_percent - y_pred)
  penalty    <- k_mean(k_sum(k_square(excess_pos) + k_square(excess_neg), axis = 1L))
  base_loss + lambda_max * penalty
}
model %>% compile(loss = dcrra_loss, optimizer = optimizer_adam(learning_rate = 5e-5))

# -------- 5. Model Training --------
early_stop <- callback_early_stopping(monitor = 'val_loss', patience = 7, restore_best_weights = TRUE)
history <- model %>% fit(
  x                = X_feats_final,
  y                = R_next_final,
  epochs           = 200,
  batch_size       = 32,
  validation_split = 0.15,
  callbacks        = list(early_stop),
  verbose          = 2
)

# -------- 6. Extract Portfolio Returns & Weights --------
pred_weights_final <- predict(model, X_feats_final)
ppp_returns_final  <- rowSums(pred_weights_final * R_next_final)
cum_ppp_final      <- cumprod(1 + ppp_returns_final) - 1

# (Optional) convert to xts
ppp_returns_xts_raw <- xts(ppp_returns_final, order.by = as.Date(train_dates_final))
# To get monthly portfolio weights:
weights_xts <- xts(pred_weights_final, order.by = as.Date(train_dates_final))
colnames(weights_xts) <- currencies

# -------- 7. (Optional) Benchmark: Naive Forward-Rate Strategy --------
# E.g. equally weighted or carry (long highest forward premium)
naive_weights <- matrix(1/N_assets, nrow = nrow(R_next_final), ncol = N_assets)
naive_returns <- rowSums(naive_weights * R_next_final)
naive_xts <- xts(naive_returns, order.by = as.Date(train_dates_final))

# -------- 8. Plot Results --------
library(ggplot2)
library(tidyr)
cum_df <- data.frame(
  Date = as.Date(train_dates_final),
  D_PPP = cumprod(1 + ppp_returns_final),
  Naive = cumprod(1 + naive_returns)
) %>% pivot_longer(-Date, names_to = "Strategy", values_to = "Cumulative_Return")

ggplot(cum_df, aes(x = Date, y = Cumulative_Return, color = Strategy)) +
  geom_line() + theme_minimal() +
  labs(title = "Cumulative Return: D-PPP vs Naive", y = "Growth of $1", x = "Date")

# --------- END OF SCRIPT ----------
```

